---
title: 'Two Parameters: Overview'
author: Mann Talati
tutorial:
  id: two-parameters-overview
output:
  learnr::tutorial:
    progressive:  true
    allow_skip::  true
runtime: shiny_prerendered
description: "Chapter 4 Tutorial: Two Parameters"
---

```{r setup, include = FALSE}
library(learnr)
library(tutorial.helpers)
library(tidyverse)
library(primer.data)
library(rstanarm)
library(skimr)

our_data <- mpg |> 
              select(cyl, year, cty)

our_data <- our_data |>
              drop_na()

ch7 <- nhanes |> 
  select(age, gender, weight, survey) |>
  filter(age >= 18, gender == "Female") |> 
  drop_na()

ch7_female <- nhanes |>
  filter(survey == 2009, gender == "Female", age >= 18) |>
  select(weight) |>
  drop_na()

fit_obj <- stan_glm(data = ch7_female,
                    weight ~ 1, 
                    family = gaussian(), 
                    refresh = 0)

newobs <- tibble(constant = 1)

pp <- posterior_predict(fit_obj,
                  newdata = newobs) |> 
       as_tibble() |> 
       mutate_all(as.numeric)

ts <- tibble(student_id = 1:4, 
             test1 = 10:13, 
             test2 = 20:23,
             test3 = 30:33) 

knitr::opts_chunk$set(echo = FALSE)
options(tutorial.exercise.timelimit = 60, 
        tutorial.storage = "local") 
```

```{r copy-code-chunk, child = system.file("child_documents/copy_button.Rmd", package = "tutorial.helpers")}
```

```{r info-section, child = system.file("child_documents/info_section.Rmd", package = "tutorial.helpers")}
```

<!-- Discuss this trick: -->

<!-- governors |> stan_glm(formula = lived_after ~ election_age, data = ., refresh = 0) -->

<!-- MT: add 0/1 outcomes -->

## Introduction
###

This tutorial covers [Chapter 4](https://ppbds.github.io/primer/07-two-parameters.html) of [*Preceptor’s Primer for Bayesian Data Science: Using the Cardinal Virtues for Inference*](https://ppbds.github.io/primer/). This tutorial will review topics including Two Parameters, Cardinal Virtues (wisdom, justice, courage, and temperance), and a 0/1 outcomes.

## Preliminaries 

### 

In this chapter, there were several functions thrown out there that might need a refresher or a proper introduction. Let's quickly familiarize ourselves with them so we are ready to take on the exercises that follow!

### 

### Exercise 1

Consider this tibble, `ts`, containing the test scores for three students on four tests. Start a pipe with `ts` and pipe it to `row_wise()`.

```{r preliminaries-1, exercise = TRUE}

```

```{r preliminaries-1-hint-1, eval = FALSE}
... |> 
  rowwise()
```

### 

`rowwise()` as well as helper functions like `c_across()` tell R to compute using the rows of a tibble.

### Exercise 2

Continue the pipe and use `mutate()` to create a new column called `avg` which is the average score (`mean()`) for each student across all three tests.

```{r preliminaries-2, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r preliminaries-2-hint, eval = FALSE}
... |> 
  mutate(... = mean(c_across(test1:test3)))
```

### Exercise 3

Start a new pipe with `ts` and pipe it to `row_wise()`. 

```{r preliminaries-3, exercise = TRUE}

```

```{r preliminaries-3-hint-1, eval = FALSE}
... |>
  ...()
```

### Exercise 4

Use `mutate()` and `c_across()` to calculate the sum (`sum()`) and standard deviation (`sd()`) of the test scores for each student.

```{r preliminaries-4, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r preliminaries-4-hint, eval = FALSE}
... |>
  ...(sum = sum(c_across(...)),
         sd = sd(c_across(...)))
```

### Exercise 5

For the remaining questions we will work with the `mpg` data set. Use `select()` to pick the variables `cyl`, `year` and `cty`. Assign the result to an object named `our_data`.

```{r preliminaries-5, exercise = TRUE}

```

```{r preliminaries-5-hint-1, eval = FALSE}
... <- mpg |> 
        select(...)
```

```{r preliminaries-5-hint-2, eval = FALSE}
our_data <- mpg |> 
              select(cyl, year, cty)
```

###

`our_data` is now available for use in later questions.

### Exercise 6

Before doing any calculations, we should get a rough overview of the data set. Run `skim()` on the tibble `our_data`.

```{r preliminaries-6, exercise = TRUE}

```

```{r preliminaries-6-hint-1, eval = FALSE}
skim(...)
```

### Exercise 7

Let's get rid of all rows with any missing values. Remove the observations with `NA`s by creating a pipe and using `drop_na()`. Save the clean tibble again as `our_data` as you did before.

```{r preliminaries-7, exercise = TRUE}

```

```{r preliminaries-7-hint-1, eval = FALSE}
... <- our_data |> 
        ...()
```

```{r preliminaries-7-hint-2, eval = FALSE}
our_data <- our_data |> 
              drop_na()
```

### Exercise 8

Since we are tech wizards, we want to make things as simple as possible. Recall the `:` operator, which creates a shortcut! Run the following code. 

```{r preliminaries-8, exercise = TRUE}
mpg |>
  select(manufacturer:trans)
```

### 

As you can see, `:` defines a closed interval. `select` returns all columns from (and including) 'manufacturer' to 'trans' in the order they appear in the tibble.

### Exercise 9

Awesome! We can also use `[ ]` to extract columns. This is especially useful when we want to extract variables not by their name, but by their position. Extract the third variable from `mpg` using this method.  

**Note:** When you use `[ ]` , place a comma before the column number you want to extract. 

```{r preliminaries-9, exercise = TRUE }

```

```{r preliminaries-9-hint-1, eval = FALSE }
mpg[...]
```

### Exercise 10

As before, we can combine `[ ]` with the `:` operator to extract multiple variables. Use this method to extract the first two variables from `mpg`.

```{r preliminaries-10, exercise = TRUE }

```

```{r preliminaries-10-hint-1, eval = FALSE }
mpg[...:...]
```

```{r preliminaries-10-hint-2, eval = FALSE }
mpg[,1:2]
```

###

Now we have successfully revisited all of the important topics for this chapter!

## Cardinal Virtues with Wisdom
###

*Wisdom* begins with the Preceptor Table. What data would we, ideally, require to answer our questions? We then explore the data that we actually have. We apply the concept of validity to ensure that the data we want and the data we have are similar enough to allow the latter to inform us about the former. If so, we describe the population from which both the Preceptor Table and out data are drawn.

### 

Let's make the graph below.

```{r}
ch7 |>
  ggplot(aes(x = weight, color = gender)) + 
  geom_density() + 
  labs(x = "Weight, in kg",
       y = "Density",
       title = "Adult Female Weight in NHANES") +
  theme(legend.position = "none")
```

### Exercise 1

Run `glimpse()` on `ch7`.

```{r wisdom-1, exercise = TRUE}

```

```{r wisdom-1-hint, eval = FALSE}
glimpse()
```

### Exercise 2

Start a pipe with `ch7` using `ggplot()`, setting `x` to `weight` and `color` to `gender` in `aes()`.

```{r wisdom-2, exercise = TRUE}

```

```{r wisdom-2-hint, eval = FALSE}
... |>
  ggplot(aes(x = ..., color = ...))
```

### Exercise 3

Add `geom_density()` to the code.

```{r wisdom-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r wisdom-3-hint, eval = FALSE}
... +
  geom_density()
```

### Exercise 4

Add the following labels with `labs()` and `theme(legend.position = "none")` to remove the legend.

```{r wisdom-4, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r wisdom-4-hint, eval = FALSE}
... + 
  labs(x = "...",
       y = "...",
       title = "...") +
  theme(... = "none")
```

### Exercise 5

As you can tell, the graph above displays "Adult Female Weight by Gender in NHANES". Let's say our motive behind generating this graph was to answer the question: _What is the probability that the next adult female we meet in Cambridge today will weigh more than 100kg?_ 

a paragraph about whether or not this data is relevant for the problem we face. See *The Primer* for guidance. Make sure to use the terms "Population" and "Preceptor Table".

Using the concept of **Wisdom**, write two sentences about what the population is in this scenario and how it is affected by wisdom.

```{r wisdom-5}
question_text(NULL,
	message = "The population must encompass both the data we have (NHANES) and the data from the preceptor table (the data we want to have). First, the population must include people from all over the world (if we are to assume that we can use United States data to answer questions about people in Cambridge — whether or not this assumption can be made, is a question reserved for Justice) and the population must include data about people in all years at least from 2009 (the earliest year we have data for).",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

###

Generally, the population will be much larger than either the data we have or the data we want. In fact, there is almost always a time dimension to consider. We generally want to make inferences about right now or about the future. By definition, the data we have is always from the past.

### Exercise 6

In two sentences, provide a reason why the *assumption of validity* might not hold.

```{r cardinal-virtues-wit-6}
question_text(NULL,
	message = "Does “height” in our Preceptor Table mean the same thing as “height” in NHANES? Almost certainly and of course, we need to be careful about mistakes like measurement differences, like centimeters in one and inches in the other. However, we can stack the two data sets together and consider them to have come from the same population.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### Exercise 7

In one sentence, provide a possible issue in which the *assumption of stability* might not hold.

```{r cardinal-virtues-wit-7}
question_text(NULL,
	message = "One possible issue could be: Are measurements taken with shoes on or shoes off?",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

## Cardinal Virtues with Justice 
###

With *Justice* we want to emphasize a few key concepts: 
1. The Population Table, a structure which includes a row for every unit in the population. We generally break the rows in the Population Table into three categories: the data for units we want to have (the Preceptor Table), the data for units which we actually have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).
2. Is our data representative of the population?
3. If the problem is causal, can we assume unconfoundedness?

### Exercise 1

$$y_i = \mu + \epsilon_i$$ \n

$$where \ \ \epsilon_i \sim N(0, \sigma^2)$$
&nbsp;

Recall the mathematical relationship above. Write a paragraph describing, in your own words, what each component of this equation describes in the context of our question. You do not need to figure out how to display the symbols in your answer, just write their names, like "phi," mu," "sigma," "epsilon," "delta," and so on.  

```{r Justice1}
question_text(NULL,
	message = "i, in this case, is an index number used to refer to a specific unit. y_i is the weight of woman number i. mu is the average weight of all women sampled. epsilon_i is the \"error term\", meaning the different between the weight of woman number i, and the average weight. epsilon_i is normally distributed with a mean of 0 and a standard deviation of sigma (variance of sigma squared). The mean being 0 relates to our concept of accuracy; we are assuming that our data is representative enough to be accurate, and so we can expect our average error to be 0. The standard deviation, on the other hand, relates to our concept of precision; the smaller sigma is, the more precise our data is, and the larger sigma is, the less precise our data is.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

### Exercise 2

Great! Next, specify which are the two parameters we want to determine. Also mention which one we care most about and why, writing no more than 2 sentences.

```{r justice-2}
question_text(NULL,
	message = "We are looking for a probability distribution for weights of adult women, so our two parameters will be the average weight (mu) and the standard deviation (sigma). Of the two, mu is far more important because we cannot conclude anything from sigma alone, but we can from mu alone.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

### Exercise 3

Using your own words, explain in 2 sentences why this is a predictive model. 

```{r justice-3}
question_text(NULL,
	message = "This model is predictive because we are not at all concerned about causation. It is predictive because there is only one Y(u) output for each unit u.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

## Cardinal Virtues with Courage 
### 

In data science, we deal with words, math, and code, but the most important of these is code. We need *Courage* to create the model, to take the leap of faith that we can make our ideas real.

###

Let's create a Bayesian model of female weight.

### Exercise 1

Start a new pipe with `nhanes`. Use `filter()` to choose rows for **adult women** surveyed in **2009**, use `select()` to select only their weights, use `drop_na()` to drop any NA values, and then assign this all to a new object called `ch7_female`.

```{r courage-1, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r courage-1-hint-1, eval = FALSE}
nhanes |> 
  filter(...) |> 
  select(...) |> 
  drop_na() -> ch7_female
```

We will use the `ch7_female` tibble in the questions which follow.

### 

### Exercise 2

Next we'll use `stan_glm()` to create a simple Bayesian model. When using `stan_glm()`, there are always four arguments that should be specified:

* The `data` argument tells `stan_glm()` what data frame to work with. In this case, we'll set it to `ch7_female`.
* The `family` argument tells `stan_glm()` what error distribution and link function to use in the model. In this case, we'll set it to `gaussian()` (which is, for our intents and purposes, synonymous with normal).
* We usually set the `refresh` argument to 0 — otherwise, the function will spit back a lot of messy lines before giving us the meat of the output.
* The `formula` argument tells `stan_glm()` what model is to be fitted. In this case, since we are working with a predictive model and not a casual model, we should set it to `weight ~ 1` (constant).

```{r courage-2, exercise = TRUE}
library(rstanarm)


```

```{r courage-2-hint-1, eval = FALSE}
stan_glm(data = ...,
         formula = ..., 
         family = ..., 
         refresh = ...)
```

### 

### Exercise 3

Next, we should save our model to be able to work with it in later questions.

Copy your code from the previous exercise, and assign it to an object called `fit_obj`.

```{r courage-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

While we have saved the model, the model summary is above still somewhat complex. Let's simplify this.

### 

### Exercise 4

Instead of printing out the whole model like before, just use `print` to print out the parameters of the model. Set the `detail` argument to FALSE.

```{r courage-4, exercise = TRUE}

```

```{r courage-4-hint-1, eval = FALSE}
print(fit_obj, detail = ...)
```

### 

### Exercise 5

Recall that the most important single number related to a distribution is some measure of its location. The two common measures for this are _mean_ and _median_. 

In a single sentence, using your own words, describe why we use the median here.

```{r courage-5}
question_text(NULL,
	message = "The median is less affected by outliers, and is therefore a more stable measure of the distribution's center.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

### Exercise 6

Now, in 2 sentences, use your own words to define `MAD_SD`.

```{r courage-6}
question_text(NULL,
	message = "The MAD_SD is the scaled median absolute deviation. It is a measure of the variability of our posterior distributions for a given parameter.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

### Exercise 7

Awesome! Now let's create the following posterior distribution.

```{r courage-7}
weight_p <- fit_obj |> 
  as_tibble() |> 
  rename(mu = `(Intercept)`) |> 
  ggplot(aes(x = mu)) +
    geom_histogram(aes(y = after_stat(count/sum(count))), 
                   bins = 200, 
                   color = "white") +
    labs(title = "Posterior Probability Distribution",
         subtitle = "Average weight among American adult women in 2009",
         x = "Weight in Kilograms",
         y = "Probability") +
    theme_classic()

weight_p
```

First, create a pipe starting with `fit_obj` and then continuing with `as_tibble()`.

```{r not_print, exercise = TRUE}

```

```{r courage-7-hint-1, eval = FALSE}
fit_obj |> 
  ...
```

### 

### Exercise 8

Cool. Now copy and paste your work from the previous question and continue the pipe. Use `rename()` and rename `(Intercept)` as `mu`. Recall that column names can be anything you want. But, if you use weird things — like a parenthesis — then you need to put backticks around the column names. Since that is a bother, we prefer column names like `mu` to column names like `(Intercept)`.

```{r courage-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r courage-8-hint-1, eval = FALSE}
fit_obj |> 
  as_tibble() |>
  rename(mu = `(Intercept)`) 
```

### 

### Exercise 9

  
Continue the pipe and use `ggplot()`. Map x to `mu` in the `aes` argument. Use `geom_histogram()` and map y to the `after_stat()` function. Set its argument to `count` divided by the `sum` of `count`.
    
```{r courage-9, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r courage-9-hint-1, eval = FALSE}
fit_obj |> 
  as_tibble() |> 
  rename(mu = `(Intercept)`) |> 
  ggplot(aes(x = ...)) +
    geom_histogram(aes(y = ...))
```

### 

### Exercise 10

Also, set `bins` to 200 and `color` to white within `geom_histogram()`.

```{r courage-10, exercise = TRUE}

```

### 

### Exercise 11

Title your histogram "Posterior Probability Distribution" with the subtitle "Average weight among American adult women in 2009". Also name the x-axis "Weight" and the y-axis "Probability". Add a layer and use `theme_classic()`. Reminder: Your plot should look something like this:

```{r courage-11}
weight_p
```

```{r classic, exercise = TRUE}

```

```{r courage-11-hint-1, eval = FALSE}
Use labs() to add labels.
```

### 

## Cardinal Virtues with Temperance 
### 

*Temperance* guides us in the use of the model we have created to answer the questions we began with. We create posteriors of quantities of interest. We should be modest in the claims we make. The posteriors we create are never the “truth.”

###

We have a model of female height in 2009. What can we do with it? Let's use it to answer out initial question: _What is the probability that the next adult female we meet in Cambridge today will weigh more than 100 kilos?_

### Exercise 1

Create a tibble with one variable, `constant`, which is set equal to the value 1. Assign this tibble to an object named `newobs`.

```{r temperance-1, exercise = TRUE}

```

```{r temperance-1-hint-1, eval = FALSE}
... <- tibble(...)
```

```{r temperance-1-hint-2, eval = FALSE}
... <- tibble(constant = 1)
```

`newobs` is available for use in later questions.

### 

### Exercise 2

Let's now obtain some estimates for the weight of a single woman. 

Use `posterior_predict` with `fit_obj` as the first argument to get some draws from the posterior distribution of our model. Set the `newdata` argument equal to the tibble `newobs` to indicate that we only want estimates for one person.

```{r temperance-2, exercise = TRUE}

```

```{r temperance-2-hint-1, eval = FALSE}
posterior_predict(fit_obj,
                  newdata = ...)
```

### 

### Exercise 3

Good job! We now have 4000 estimates for a random woman's weight in kg.

Copy your code from above and add two pipes with `as_tibble()` and `mutate_all(as.numeric)`. Assign the result to an object named `pp` to save it.

```{r temperance-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r temperance-3-hint-1, eval = FALSE}
... <- posterior_predict(fit_obj,
                         newdata = newobs) |> 
       ... |> 
       ...
```

`pp` is available for use in later questions.

### 

### Exercise 4

It is always helpful to plot our data before working with it.

Start a new pipe with `pp`. Create a histogram with `geom_histogram()`. Use `after_stat()` to display the y-axis in relative proportions of each weight group. Set `bins` to 200 and `color` to "white".

```{r temperance-4, exercise = TRUE }

```

```{r temperance-4-hint-1, eval = FALSE}
pp |> 
  ggplot(aes(x = ...)) +
    geom_histogram(aes(y = after_stat(...)),
                   bins = ...,
                   color = ...)
```

### 

### Exercise 5

Your code should now look as shown below.

Add `labs()` to title your histogram "4000 Draws from Posterior Distribution" with the subtitle, "Average weight of a random American adult woman in 2009". Name the x-axis "Weight in Kilograms" and the y-axis "Probability". Finally, add a layer and use `theme_classic()`. 

```{r temperance-5, exercise = TRUE}
pp |> 
  ggplot(aes(x = `1`)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 200,
                   color = "white")
```

```{r temperance-5-hint-1, eval = FALSE}
pp |> 
  ggplot(aes(x = `1`)) +
    geom_histogram(aes(y = after_stat(count/sum(count))), 
                   binwidth = 0.02, 
                   color = "white") +
    labs(title = ...,
         subtitle = ...,
         x = ...,
         y = ...) +
    theme_classic()
```

How does the distribution of our 4000 estimates compare to the actual posterior distribution you plotted earlier? As a reminder, it looked like this:

```{r courage_plot2}
fit_obj |> 
  as_tibble() |> 
  rename(mu = `(Intercept)`) |> 
  ggplot(aes(x = mu)) +
    geom_histogram(aes(y = after_stat(count/sum(count))), 
                   bins = 200, 
                   color = "white") +
    labs(title = "Posterior Probability Distribution",
         subtitle = "Average weight among American adult women in 2009",
         x = "Weight in Kilograms",
         y = "Probability") +
    theme_classic()
```

### 

### Exercise 6

Recall our question: _What is the probability that the next adult female we meet in Cambridge today will weigh more than 100 kilos?_ 

The histogram already shows approximately what the answer to our question might be, but let's calculate it exactly.

Using one line of code, determine the proportion of all women in `pp` who weigh more than 100kg. 

```{r temperance-6, exercise = TRUE}

```

```{r temperance-6-hint-1, eval = FALSE}
sum(... > 100) / length(...)
```

```{r temperance-6-hint-2, eval = FALSE}
sum(pp$`1` > 100) / length(pp$`1`)
```

## Summary
###

This tutorial covered [Chapter 4](https://ppbds.github.io/primer/07-two-parameters.html) of [*Preceptor’s Primer for Bayesian Data Science: Using the Cardinal Virtues for Inference*](https://ppbds.github.io/primer/). This tutorial reviewed the topics of Two Parameters, Cardinal Virtues (wisdom, justice, courage, and temperance), and a 0/1 outcomes.

```{r download-answers, child = system.file("child_documents/download_answers.Rmd", package = "tutorial.helpers")}
```
