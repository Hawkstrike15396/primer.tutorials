---
title: 'One Parameter: Polling'
author: Ryan Southward
tutorial:
  id: one-parameter-polling
output:
  learnr::tutorial:
    progressive: yes
    allow_skip: yes
runtime: shiny_prerendered
description: Use polling data from the 2013 election to answer questions about support
  for President Obama.
---

```{r setup, include = FALSE}
library(learnr)
library(all.primer.tutorials)
library(tidyverse)
library(primer.data)
library(rstanarm)
library(skimr)
library(gt)
knitr::opts_chunk$set(echo = FALSE)
options(tutorial.exercise.timelimit = 60, 
        tutorial.storage = "local") 

# Create a fake data set with polling data.

# Harvard IOP poll w/ 2,089 respondents: 
set.seed(17)
iop_data <- tibble(response = c(rep("Approve", 855),
                                 rep("Disapprove", 1124),
                                 rep(NA, 110))) |> 
  sample_frac()

#Class poll w/ 200 respondents
class_data <- tibble(response = c(rep("Approve", 64),
                                 rep("Disapprove", 104),
                                 rep(NA, 32))) |> 
  sample_frac()

#Clean data to create fit. Have students use originial data set and drop NA. 

iop_clean <- iop_data |>
  drop_na() |>
  mutate(approve = if_else(response == "Approve", 1, 0)) |>
  select(approve)

class_clean <- class_data |>
  drop_na() |>
  mutate(approve = if_else(response == "Approve", 1, 0)) |>
  select(approve)

# Posterior creation

# iop_fit <- stan_glm(data = iop_clean,
#                     refresh = 0,
#                     seed = 2013,
#                     family = binomial,
#                     formula = approve ~ 1)
# 
# write_rds(iop_fit, "data/iop_fit.rds")

# Create model: 
iop_fit <- read_rds("data/iop_fit.rds")

# Create graph of posterior: 
ob_iop_ppd <- posterior_epred(iop_fit, 
                          newdata = tibble(constant = 1)) |>
  as_tibble() |>
  rename(approval = `1`) |>
  mutate(poll = "IOP poll")

# class_fit <- stan_glm(data = class_clean,
#                     refresh = 0,
#                     seed = 2013, 
#                     family = binomial,
#                     formula = approve ~ 1) 

# write_rds(class_fit, "data/class_fit.rds")

# Create model: 
class_fit <- read_rds("data/class_fit.rds")

# Create graph of posterior: 
class_ppd <- posterior_epred(class_fit, 
                          newdata = tibble(constant = 1)) |>
  as_tibble() |>
  rename(approval = `1`) |>
  mutate(poll = "Class poll")

# Standard error

iop_sd <- sd(iop_clean$approve)

# Posterior predictive distributions for both IOP and class polls:

iop_predictions <- posterior_predict(iop_fit, 
                                     newdata = tibble(constant = rep(1, 20))) |>
  as_tibble() |>
  mutate(total = rowSums(across(`1`:`20`))) |>
  select(total) |>
  mutate(poll = "IOP poll")

class_predictions <- posterior_predict(class_fit, 
                                     newdata = tibble(constant = rep(1, 20))) |>
 as_tibble() |>
  mutate(total = rowSums(across(`1`:`20`))) |>
  select(total) |>
  mutate(poll = "Class poll")
  
```

```{r copy-code-chunk, child = "../../child_documents/copy_button.Rmd"}

```

```{r info-section, child = "../../child_documents/info_section.Rmd"}

```

<!-- A future editor may choose to make the small survey and large survey have the same population proportion to illustrate this point. However, different population proportions is necessary for the "Smaller sample" exercise.  -->

<!-- 3) When comparing two surveys of different sizes, inferences about future samples differ much less than you might expect. Main difference is that smaller survey predicts that outlier events are (very slightly!) more like likely.  -->

<!-- RS: Because the second survey we use in this tutorial has a different value of p, it would be difficult to prove this point. -->

<!-- Be careful about how the term population parameter is used, because it is rarely that what our model predicts and the population parameter are the same thing.  -->

## Introduction
### 

In this section we will examine polling using presidential approval polls. First, you will examine data from a 2013 approval poll for President Obama conducted by the Harvard Kennedy Institute of Politics, using *Wisdom* and *Justice* to determine whether or not it can answer our questions. Next, you will calculate feasible values for the percentage of young Americans who approved of President Obama by creating a posterior distribution using the professional **rstanarm** statistical library. You will then compare the posterior of the Harvard IOP poll with another poll conducted by a government class using stastical methods. Finally, you will use the posteriors you create to find the expected predictions of a future situation. 

## Wisdom
### 

Before creating models, use *Wisdom* to determine whether or not the data you have is close enough to the data you need to answer your question.

### 

Our question: 

> Imagine that it is November 15, 2013. Our question: What percentage of young people (aged 18 to 29) approve of President Obama's performance?  We will use the 2013 Harvard Institute of Politics (IOP) polling data collected between October 29 and November 13 to answer the question.

### Exercise 1

In two sentences, describe the Preceptor Table which could be used to answer our question. Specify what types of people would be included in the rows, and the necessary columns of data to answer the question. As a hint, there should be two columns.

```{r wisdom-1}
question_text(NULL,
	message = "The Preceptor Table includes a row for every person in the US who is 18 to 29. It has two columns: an 'ID' column and an 'approval' column which indicates whether or not the person approved, on November 15, of President Obama's performance.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Because our question asks us for President Obama's approval rating on *November 15, 2013*, the data to fill in our Preceptor Table should come from that exact day. 

### Exercise 2

In one sentence, explain whether this model is causal or predictive, and why.

```{r wisdom-2}
question_text(NULL,
	message = "The model is predictive because there is only one potential outcome for each person: their approval (or not), November 15, 2015, of President Obama's performance.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Because our model is predictive, we only have one outcome column. 

### Exercise 3

Between October 29 and November 13 of 2013, a team from Harvard's Institute of Politics [conducted their Fall 2013 Poll](https://iop.harvard.edu/sites/default/files_new/Harvard_PressReleaseFall2013%20.pdf) **exclusively** polling 2,089 young Americans aged 18 to 29. Type `iop_data` to view the results of the poll.   


```{r wisdom-3, exercise = TRUE}

```

### 

The data we use comes from the yearly Harvard Public Opinion Project conducted by the Harvard Kennedy School Institute of Politics: 

> The Harvard Public Opinion Project conducts a biannual poll examining the political opinions and civic engagement of young Americans ages 18 to 29. Since its conception by two Harvard undergraduate students in 2000, the Harvard Public Opinion Project has provided the most comprehensive look at the political opinions, voting trends, and views on public service held by young Americans (iop.harvard.edu).

### Exercise 4

In the results we appear to see three possible responses: approve, disapprove, or NA. Let's use `skim()` from the **skimr** package on `iop_data` to confirm this. 


```{r wisdom-4, exercise = TRUE}

```

```{r wisdom-4-hint-1, eval = FALSE}
skim(...)
```

### 

Looking at n_unique for our single character column, we see that there are only 2 unique values. We also see that there are 110 missing values. This corresponds with our initial belief that there are only 3 possible values: "Approve", "Disapprove", or `NA`. Always take a thorough look at your data to make sure there are not hidden levels that are not visible at first glance. 

### Exercise 5

Although `skim()` identifies how many levels each column in our data has, it does not display the frequencies of each level. To do this, start a pipe with `iop_data`, and pipe on the function `count()`. Within `count()`, use the sole argument `response`. 


```{r wisdom-5, exercise = TRUE}

```

```{r wisdom-5-hint-1, eval = FALSE}
... |>
  count(...)
```

### 

In effect we have created a summary of the Harvard IOP poll: Out of the 2,089 poll respondents, 855  approved of President Obama, 1124 disapproved, and answers are missing from 110. 

### Exercise 6

The final question of Wisdom is always can we can consider the Preceptor Table and the data to come from the same population? If we cannot assume *validity* across columns, meaning that that the "approval" column of the Preceptor Table does not match the meaning of the approval column of the data, then we conclude that they do not come from the same Population. 

In a paragraph, challenge the *validity* of the combining the Preceptor Table and the data together. 


```{r wisdom-6}
question_text(NULL,
	message = "The column labelled 'response' in the data is not exactly the same thing as the column labeled 'approval' in the Preceptor Table. 'response' in the data refers to what people said on the phone to strangers. This is not really what we care about! People sometimes (often?) lie to strangers! The 'approval' column in the Preceptor Table refers to people's honest internal feelings. If those are not the same thing, then the assumption of 'validity' does not hold and we can not consider the Preceptor Table and the data to have come from the same population.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

In order to assume validity and proceed with our question, we must assume that the question asked in the survey accurately gauges people's attitudes towards President Obama. We must also assume that other aspects of experimental design, such as whether or not people answered the questions truthfully, are not an issue.

## Justice
### 

The next virtue after Wisdom is Justice. Given a Population Table, we need to examine the assumptions of stability and representativeness. We also need to specify the functional form of the Data Generating Mechanism.

### Exercise 1

In one sentence, state the columns that the Population Table should have. Hint: there should be 4. 

```{r justice-1}
question_text(NULL,
	message = "ID, Source, Time, Approval",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Every Population Table has a source column that identifies if the subject is from the Data, Preceptor Table, or Population, a time column, as well as at least one outcome column. 

###

Your Population Table would look like this: 

```{r}
tibble(source = c("..."),
       time = c("..."),
       id = c("..."),
       approval = c("...")) |>

  # Then, we use the gt function to make it pretty

  gt() |>
  cols_label(source = md("Source"),
             time = md("Time"),
             id = md("ID"),
             approval = md("Approval")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(source))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"),
            locations = cells_column_labels(columns = c(source))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(source)) |>
  fmt_markdown(columns = everything())
```


### Exercise 2

Describe in one sentence the rows from the Preceptor Table which are in the Population Table. Make sure to reference *time* in some capacity in your response. 

```{r justice-2}
question_text(NULL,
	message = "The Preceptor Table would have as many rows as there are 18-29 year old Americans on Nov 15, 2013, in order to make the simple algebraic calculation of President Obama's approval among all young Americans.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

It is important to state that we care about the attitudes of young Americans specifically on November 15, 2013, as that is the date the question asks us, and we acknowledge that the number of Americans aged 18-29 will change from day to day. 

###

Your Population Table would look like this: 

```{r}
tibble(source = c("Preceptor Table", "Preceptor Table", "Preceptor Table", "..."),
       time = c("November 15, 2013", "November 15, 2013", "November 15, 2013", "..."),
       id = c("574-10-8912", "505-33-0002", "700-58-3054", "..."),
       approval = c("?", "?", "?", "...")) |>

  # Then, we use the gt function to make it pretty

  gt() |>
  cols_label(source = md("Source"),
             time = md("Time"),
             id = md("ID"),
             approval = md("Approval")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(source))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"),
            locations = cells_column_labels(columns = c(source))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(source)) |>
  fmt_markdown(columns = everything())
```

### Exercise 3

Describe in one sentence the rows from the data which are in the Population Table. 

```{r justice-3}
question_text(NULL,
	message = "The Data source would have 1,979 rows, one for each poll respondent who had an opinion on wether or not they approved of President Obama.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

You may have answered that there should be 2,089 rows, which is a perfectly reasonable response. The answer is in fact 1,979 rows because the people who declined to answer (represented by NAs in our data), do not give us data which can be used to answer our question.

###

Your Population Table would look like this: 

```{r}
tibble(source = c("Data", "Data", "Data", "...",
                  "Preceptor Table", "Preceptor Table", "Preceptor Table", "..."),
       time = c("October 30, 2013", "November 2, 2013", "November 8, 2013", "...",
         "November 15, 2013", "November 15, 2013", "November 15, 2013", "..."),
       id = c("574-10-8912", "510-01-7888", "486-98-8213", "...",
         "574-10-8912", "505-33-0002", "700-58-3054", "..."),
       approval = c("Approve", "Disapprove", "Approve", "...", 
         "?", "?", "?", "...")) |>

  # Then, we use the gt function to make it pretty

  gt() |>
  cols_label(source = md("Source"),
             time = md("Time"),
             id = md("ID"),
             approval = md("Approval")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(source))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"),
            locations = cells_column_labels(columns = c(source))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(source)) |>
  fmt_markdown(columns = everything())
```

### Exercise 4

Describe in one sentence the rows from the wider population which are in the Population Table. Make sure to reference *time* in some capacity in your response. 

```{r justice-4}
question_text(NULL,
	message = "The Population would be larger than the Preceptor Table and have rows for all young Americans aged 18-29, including multiple rows for a single individual as their opinion may have changed.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

There should also be rows for people who may turn 18 a few days after the poll, and also people who turned 30 after the poll was conducted. We must acknowledge all members of the wider population that we *could* have polled, but for some reason did not. 

###

Your final Population Table would look like this: 

```{r}
tibble(source = c("Population", "Population", "Population", "...",
                  "Data", "Data", "Data", "...",
                  "Population", "Population", "Population","...",
                  "Preceptor Table", "Preceptor Table", "Preceptor Table", "...",
                  "Population", "Population", "Population","..."),
       time = c("October 25, 2013", "October 26, 2013", "November 26, 2013", "...",
         "October 30, 2013", "November 2, 2013", "November 8, 2013", "...",
         "October 30, 2013", "November 4, 2013", "November 11, 2013", "...",
         "November 15, 2013", "November 15, 2013", "November 15, 2013", "...", 
         "November 14, 2013", "November 15, 2013", "November 16, 2013", "..."),
       id = c("574-10-8912", "499-23-1005", "543-70-9810", "...",
         "574-10-8912", "510-01-7888", "486-98-8213", "...",
         "574-10-8912", "501-12-5767", "555-73-2312", "...",
         "574-10-8912", "505-33-0002", "532-58-3054", "...",
         "574-10-8912", "494-88-9852", "483-22-1076", "..."),
       approval = c("?", "?", "?", "...", 
         "Approve", "Disapprove", "Approve", "...", 
         "Approve", "?", "?", "...", 
         "?", "?", "?", "...", 
         "?", "?", "?", "...")) |>

  # Then, we use the gt function to make it pretty

  gt() |>
  cols_label(source = md("Source"),
             time = md("Time"),
             id = md("ID"),
             approval = md("Approval")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(source))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"),
            locations = cells_column_labels(columns = c(source))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(source)) |>
  fmt_markdown(columns = everything())
```

### Exercise 5

In one sentence, state a component of the Population Table that would have unknown values and explain those values would be unknown.   

```{r justice-5}
question_text(NULL,
	message = "The approval outcome of Preceptor Table subjects would be unknown because we have no data on the attitudes of young Americans on November 15, 2013.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

For the Population source, we would also know the approval outcome of 2,089 members whose data we collected in the poll, but the vast majority of young Americans approval of Obama would be unknown, including the attitudes of poll respondents both before and after the poll was conducted as their opinions could have changed.

### Exercise 6

The Harvard IOP poll was conducted between October 29 and November 13 of 2013. In 2 sentences, state a reason why the assumption of *stability* might not be met in this exercise. 

```{r justice-6}
question_text(NULL,
	message = "A major news event that happened after the poll was conducted could cause the attitudes of young adults to shift dramatically. If this were to be the case, the poll would not be representitive of the respondents more current beliefs, and instead only reflect their past attitudes before the news event.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

For presidential approval polls, time essentially creates a new population each day. For example, from October 1st to October 17th, 2013, the U.S government shut down under President Obama causing his approval rating to dip. Political events influence people's day to day beliefs, and it is fairly certain that some people would have differing attitudes about President Obama during and after the shutdown, showing that individuals from these two time periods could be considered entirely different populations. 

### Exercise 7

The [press release revealing the poll results](https://iop.harvard.edu/sites/default/files_new/Harvard_PressReleaseFall2013%20.pdf) included a brief explanation of KnowledgePanel®, which was the platform used to conduct the poll:

> The web-enabled KnowledgePanel® is a probability-based panel designed to be representative of the
U.S. population. Initially, participants are chosen scientifically by a random selection of 
telephone numbers and residential addresses. Persons in selected households are then invited by
telephone or by mail to participate in the web-enabled KnowledgePanel®. For those who agree
to participate, but do not already have Internet access, GfK provides a laptop and ISP connection
at no cost. 

With knowledge of the sampling methodology, determine one group the poll may exclude (and explain why it excludes them), which could cause the poll to not be *representative* of all 18-29 year old Americans. Respond in 1 sentence. 

```{r justice-7}
question_text(NULL,
	message = "The poll would not represent 18-29 year old prisoners, as they could not be selected via random selection of telephone numbers and addresses.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Almost no sample in the real world is ever perfectly representative. In fact, *representativeness is always a lie* to a certain extent, and for almost all surveys *some* group may be excluded. However, just because a survey is not 100% representative doesn't mean that its results can't be used to draw conclusions. 

### Exercise 8

Good. Now let's turn our attention to the Data Generating Mechanism (DGM). Should our polling scenario be modeled by a normal or binomial distribution? Explain your reasoning in 2 sentences.  

```{r justice-8}
question_text(NULL,
	message = "Our polling scenario should be modeled by a binomial distribution, because we model the number of people who approve of President Obama (successes) over people who disapprove of him (failures). Because there are only 2 possibilities to model, the poll can be modeled by a binomial distribution",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

To create our models we will use the following binomial model: 

$$ T_a \sim B(\rho , n = 2089)$$

The total number of young Americans aged 18-29 who approve of President Obama, $T_a$, is modeled by a binomial distribution with the probability of one young American approving of President Obama, $\rho$, and 2,089 observations. 

### 

Use Justice to further increase your understanding of the situation at hand. Justice always consists of a Population Table, stability, representativeness, and the Data Generating Mechanism.


## IOP Study
### 

Whenever you want to answer a question, create a posterior distribution. 

### 

Recall our question: 

> Imagine that it is November 15, 2013. What percentage of young people (aged 18 to 29) approve of President Obama's performance?

### 

In order to answer the question, use the Harvard IOP poll to create the posterior distribution for the percentage of U.S 18-29 year olds who approved of President Obama in the fall of 2013. 

```{r}
ob_p <- ob_iop_ppd |>
  ggplot(aes(x = approval)) +
  geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_light() +
  labs(title = "Posterior Probability Distribution",
       x = "Percentage of 18-29 year olds who approved of President Obama on Nov 15, 2013",
       y = "Probability")

ob_p 
```

### Exercise 1

In order to create a binomial model, you must structure your data so that 1's represent successes and 0's represent failures. For structuring the responses for the Obama poll, it makes sense to represent those who approved of Obama's performance with 1's, and those who disapproved with 0's. 

Run `iop_data` to view the poll results once more. 

```{r iop-study-1, exercise = TRUE}

```

### Exercise 2

Why are there some NA values? Some respondents of the Harvard IOP poll declined to answer whether or not they approved of President Obama. Assuming the absence of non-response bias, NA values should be dropped to focus our attention on the ratio between approvals and disapprovals. Start a pipe with `iop_data` and use `drop_na()`. 

```{r iop-study-2, exercise = TRUE}

```

```{r iop-study-2-hint-1, eval = FALSE}
... |>
  drop_na()
```

### 

Your tibble should now have 1,979 rows, representing the 2,089 total poll respondents without the 110 respondents who declined to answer the question. 

### Exercise 3

Our data entries are either "Approve" or "Disapprove", instead of 1's and 0's. To change our data to a binomial format, create a new column called `approve` and set it equal to the `if_else()` function. For the first argument, pass in a logical comparison statement that checks if the `response` is equal to "Approve". For the second argument use 1 and for the third argument use 0.

```{r iop-study-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-3-hint-1, eval = FALSE}
... |>
  mutate(... = if_else(... == ..., ..., ...))
```

```{r iop-study-3-hint-2, eval = FALSE}
... |>
  mutate(... = if_else(... == "Approve", 1, 0))
```

### 

Rows with 1's represent respondents who approved of President Obama's performance, and 0's represent those who disapproved. 

### Exercise 4

Finally, select the `approve` column, and assign your code from above to an object named `iop_clean`.

```{r iop-study-4, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-4-hint-1, eval = FALSE}
Use the assignment operator <-  to do so.
```

### 

Excellent work. Now it's time to use the **rstanarm** package to fit a model for the data. 

### Exercise 5

Next we'll use `stan_glm()` to create a simple Bayesian model. When using `stan_glm()`, there are always four arguments that should be specified:

* The `data` argument tells `stan_glm()` what data frame to work with. Set `data` to `iop_clean`. 
* The `family` argument tells `stan_glm()` what mathematical formula should be used to create our model. Set `family` to `binomial`.
* The `formula` argument tells `stan_glm()` what model is to be fitted. Because we have no predictors, we use the argument `formula = approve ~ 1`, which means that we only model the outcome based on the `approve` column of our data. 
* Set `refresh` to 0 in order to suppress the behavior of printing to the console, and `seed` to 2013 in order to get the same output every time we run the code.

```{r iop-study-5, exercise = TRUE}

```

```{r iop-study-5-hint-1, eval = FALSE}
stan_glm(data = ...,
         family = ...,
         formula = ..., 
         refresh = ...,
         seed = ...)
```

### 

You do not need to understand what the printed model means. 

### Exercise 6

Next, assign your model to an object named `iop_fit` to save it for future use.  

```{r iop-study-6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-6-hint-1, eval = FALSE}
Use the assignment operator <-  to do so.
```

### 

It's always a good idea to save your posteriors, as they will always be utilized in predictive functions like `posterior_predict()` needed to answer our questions. 

### Exercise 7

Let's recreate our posterior distribution using `posterior_epred()`, assigning our code to a variable called `ob_iop_ppd`. Pass in `iop_fit` as the first argument, and then set the argument `newdata` equal to a `tibble()`. Within the `tibble()`, include `constant = 1`. 

```{r iop-study-7, exercise = TRUE}

```

```{r iop-study-7-hint-1, eval = FALSE}
ob_iop_ppd <- posterior_epred(..., 
                 newdata = ...)
```

### 

We use `constant = 1` as junk data to make sure our tibble has just 1 row. 

### Exercise 8

Copy your code from above. First pipe `as_tibble()` to your code. Then continue your pipe by renaming the column of the resulting tibble from `1` to `approval` by using `approval = 1` within `rename()`. **Remember to include backticks around 1**. 

```{r iop-study-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-8-hint-1, eval = FALSE}
... |>
  ... |>
  rename(...)
```

```{r iop-study-8-hint-2, eval = FALSE}
Within rename, set approval equal to `1`. Make sure to include backticks around 1!
```

### 

An annoying problem with `posterior_epred()` is that names its columns weirdly. We change the name of the tibble from "1" to make it less confusing. 

### Exercise 9

Finally, continue your pipe with `mutate()`. Within `mutate()`, set `poll` to "IOP poll". We uniformly assign "IOP poll" to each row in our tibble in order to compare posteriors later on. 

```{r iop-study-9, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-9-hint-1, eval = FALSE}
... |>
  mutate(... = "IOP poll")
```

### Exercise 10

Type `ob_iop_ppd` below to view 4,000 draws from your posterior. 

```{r iop-study-10, exercise = TRUE}

```

### Exercise 11

Awesome! Now let's graph the draws from the posterior. First, start a pipe with `ob_iop_ppd`, and using `ggplot()` map `approval` to the x axis. Also add the layer `geom_histogram()`. 

```{r iop-study-11, exercise = TRUE}

```

```{r iop-study-11-hint-1, eval = FALSE}
ob_iop_ppd |>
  ggplot(aes(... = ...)) + 
  geom_histogram()
```

### 

Because we want to find the probability of each possible Obama approval rating, we should convert our y-axis to percentage values. We should also specify the number of bins to remove the warning. 

### Exercise 12

Within `geom_histogram()`, use `aes()` to set `y` equal to `after_stat(count/sum(count))` in order to convert the y-axis to percent values. Also set `bins` to 100. 

```{r iop-study-12, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-12-hint-1, eval = FALSE}
... +
  geom_histogram(aes(... = ...),
                 bins = ...)
```

### 

What is this plot missing? Shouldn't the the number on the x and y axes be percentages instead of simple numbers?

### Exercise 13

To correct the axises,  add the layer `scale_x_continuous()` with the argument `labels` set to `scales::percent_format()`. Also add the layer `scale_y_continuous()`, with the argument `labels` also set to `scales::percent_format()`. 

```{r iop-study-13, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-13-hint-1, eval = FALSE}
... +
  scale_x_continuous(... = ...) +
  scale_y_continuous(... = ...)
```

### 

To make our graph the most presentable possible, we should remove the extra unneeded decimal places for our percentages. 

### Exercise 14

Within *both* occurrences of `scales::percent_format()`, set `accuracy` to 1 in order to limit percentages to round the first decimal place. 

```{r iop-study-14, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-14-hint-1, eval = FALSE}
... +
  scale_x_continuous(labels = scales::percent_format(accuracy = ...)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = ...))
```

### Exercise 15

To finish, use `labs()` to give your graph the appropriate title, subtitle, and axis labels. Also add the layer `theme_light()`. 

```{r iop-study-15, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

Reminder: Your plot should look something like this:

```{r}
ob_p 
```

### Exercise 16

Using the posterior, answer the initial question in 2 sentences: 

> Imagine that it is November 15, 2013. What percentage of young people (aged 18 to 29) approve of President Obama's performance?

Because the exact value of the parameter (the percentage of young Americans who approved of Obama's performance) is unknown, your answer should not be a single number, but instead a range of values that the true value of the parameter is likely to fall between.  

```{r iop-study-16}
question_text(NULL,
	message = "The bulk of the area under the distribution occurs between 42% and 44%, so it's likely that the percentage of all young American adults who approved of President Obama is between this range. However, there is also some chance that the percentage is as low as 40%, and as high as 46.5%.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Whenever you want to answer a question, create a posterior distribution. 

<!-- ## Predicting outcomes -->

<!-- Lesson: Posterior predictions are never the truth.  -->

<!-- Problem with teaching lesson: Because our posterior is very refined, the predictions are pretty accurate. If you uncomment the code below, you will see 5% is the largest difference between actual and predicted.  -->

<!-- ```{r} -->
<!-- min(ob_iop_ppd$approval) -->
<!-- max(ob_iop_ppd$approval) -->

<!-- iop_pred <- posterior_predict(iop_fit, newdata = tibble(constant = rep(1, 10))) |> -->
<!--   as_tibble() |>  -->
<!--   mutate(total = rowSums(across(`1`:`10`))) |>  -->
<!--   select(total) -->

<!-- tibble(pop_pct = seq(from = .4, to = .47, by = .01)) |> -->
<!--   mutate(n_ppl = map(pop_pct, ~ 0:10)) |> -->
<!--   unnest(n_ppl) |> -->
<!--   mutate(real_prob = map_depth(.x = n_ppl, .depth = 0, ~ dbinom(x = .x, size = 10, prob = pop_pct))) |> -->
<!--   mutate(est_prob = map_dbl(n_ppl, ~ sum(iop_pred$total == .)/4000)) |> -->
<!--   mutate(diff = real_prob - est_prob) |> -->
<!--   mutate(pos = ifelse(diff > 0, TRUE, FALSE)) |> -->
<!--   ggplot(aes(x = n_ppl, y = diff, fill = pos)) +  -->
<!--   geom_col() +  -->
<!--   facet_wrap(~pop_pct) +  -->
<!--   scale_fill_manual(values = c("red", "green")) -->

<!-- ``` -->

## Smaller sample study
### 

The center of a distribution approximates a parameter for an entire population, while the spread of a distribution represents the uncertainty surrounding the estimated parameter. In our polling scenario, the proportion determines the center, while the number of observations in the sample determines the spread.

We will use this methodology of comparing distributions to eventually compare the posterior probability distributions of the Harvard IOP poll and a new Gov't class poll:

```{r}
compare_p <- bind_rows(ob_iop_ppd, class_ppd) |>
  ggplot(aes(x = approval, fill = poll)) +
  geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100, alpha = .5, position = "identity") + 
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_light() +
  labs(title = "Comparing Posterior Distributions of 2 Studies", 
       subtitle = "The differences between the studies can be expressed by
       the posterior means and spreads.",
       x = "Approval Percentage", 
       y = "Probability")

compare_p
```

### 

Also between October 29 and November 13, a high school government class conducted their own Obama approval poll using the exact same methodology as the Harvard Institute of Politics poll, the only difference being that they polled 200 respondents instead of the 2,089 polled by the Harvard IOP. 

### Exercise 1

Type `class_data` to review the results of the class poll. 

```{r smaller-sample-study-1, exercise = TRUE}

```

### 

There appear to be 3 possible answers: approve, disapprove, or NA. 

### Exercise 2

Let's use the **skimr** package to see how many levels there are in the `response` column. Use `skim()`, passing in `class_data`. 

```{r smaller-sample-study-2, exercise = TRUE}

```

```{r smaller-sample-study-2-hint-1, eval = FALSE}
skim(...)
```

### 

Looking at n_unique for our single character column, we see that there are only 2 unique values. We also see that there are 32 missing values. This corresponds with our initial belief that there are only 3 possible values: approve, disapprove, or NA. Always take a thorough look at your data to make sure there are not hidden levels that are not visible at first glance. 

### Exercise 3

Although `skim()` identifies how many levels each column in our data has, it does not display the frequencies of each level. To do this, start a pipe with `class_data`, and pipe on the function `count()`. Within `count()`, use the sole argument `response`. 


```{r smaller-sample-study-3, exercise = TRUE}

```

```{r smaller-sample-study-3-hint-1, eval = FALSE}
... |>
  count(...)
```

### 

We can now easily view the results of the class poll: Out of the 200 poll respondents, 64  approved of President Obama, 104 disapproved, and answers are missing from 32.

### Exercise 4

Because people who failed to answer the question do not provide data on their attitude towards President Obama, they should be dropped. Start a pipe with `class_data` and pipe on `drop_na()`

```{r smaller-sample-study-4, exercise = TRUE}

```

```{r smaller-sample-study-4-hint-1, eval = FALSE}
... |>
  drop_na()
```

### 

Your tibble should now have 168 rows, representing the 200 total poll respondents without the 110 respondents who declined to answer the question. 

### Exercise 5

In order to create a binomial model, the next step is to transform the poll summary into a binomial format. Those who approved of Obama's performance should be represented with 1's, and those who disapproved with 0's. Create a new column called `approve` and set it equal to the `if_else()` function. For the first argument, pass in a logical comparison statement that checks if the `response` is equal to "Approve". For the second argument use 1 and for the third argument use 0.

```{r smaller-sample-study-5, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-5-hint-1, eval = FALSE}
... |>
  mutate(... = if_else(... == ..., ..., ...))
```

```{r smaller-sample-study-5-hint-2, eval = FALSE}
... |>
  mutate(... = if_else(... == "Approve", 1, 0))
```

### 

Now functions like `posterior_predict()` can understand our model. Rows with 1's represent respondents who approved of President Obama's performance, and 0's represent those who disproved. 

### Exercise 6

Assign your code from above to an object named `class_clean`.

```{r smaller-sample-study-6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-6-hint-1, eval = FALSE}
Use the assignment operator <-  to do so.
```

### 

Excellent work. Now it's time to use the **rstanarm** package to fit a model for the data. 

### Exercise 7

Next we'll use `stan_glm()` to create a simple Bayesian model. When using `stan_glm()`, there are always four arguments that should be specified:

* The `data` argument tells `stan_glm()` what data frame to work with. Set `data` to `class_clean`. 
* The `family` argument tells `stan_glm()` what mathematical formula should be used to create our model. Set `family` to `binomial`.
* The `formula` argument tells `stan_glm()` what model is to be fitted. Because we have no predictors, we use the argument `formula = approve ~ 1`, which means that we only model the outcome based on the `approve` column of our data. 
* Set `refresh` to 0 in order to suppress the behavior of printing to the console, and `seed` to 2013 in order to get the same output every time we run the code.

```{r smaller-sample-study-7, exercise = TRUE}

```

```{r smaller-sample-study-7-hint-1, eval = FALSE}
stan_glm(data = ...,
         family = ...,
         formula = ..., 
         refresh = ...,
         seed = ...)
```

### 

You do not need to understand what the printed model means. 

### Exercise 8

Next, assign your model to an object named `class_fit` to save it for future use.  

```{r smaller-sample-study-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-8-hint-1, eval = FALSE}
Use the assignment operator <-  to do so.
```

### Exercise 9

Let's recreate our posterior distribution using `posterior_epred()`, assigning our code to a variable called `class_ppd`. Pass in `class_fit` as the first argument, and then set the argument `newdata` equal to a `tibble()`. Within the `tibble()`, include `constant = 1`. 

```{r smaller-sample-study-9, exercise = TRUE}

```

```{r smaller-sample-study-9-hint-1, eval = FALSE}
class_ppd <- posterior_epred(..., 
                 newdata = ...)
```

### 

We use `constant = 1` as junk data to make sure our tibble has just 1 row. 

### Exercise 10

Copy your code from above. First pipe `as_tibble()` to your code. Then continue your pipe by renaming the column of the resulting tibble from `1` to `approval` by using `approval = 1` within `rename()`. **Remember to include backticks around 1**. 

```{r smaller-sample-study-10, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-10-hint-1, eval = FALSE}
... |>
  ... |>
  rename(...)
```

```{r smaller-sample-study-10-hint-2, eval = FALSE}
Within rename, set approval equal to `1`. Make sure to include backticks around 1!
```

### Exercise 11

Finally, continue your pipe with `mutate()`. Within `mutate()`, set `poll` to "IOP poll". 

```{r smaller-sample-study-11, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-11-hint-1, eval = FALSE}
... |>
  mutate(... = ...)
```

### 

We uniformly assign "IOP poll" to each row in our tibble in order to compare posteriors later on.

### Exercise 12

Type `class_ppd` below to view 4,000 draws from the posterior. 

```{r smaller-sample-study-12, exercise = TRUE}

```

### 

Now that we have the posterior draws for the class poll, we can work to compare it to the posterior of the IOP poll. 

### Exercise 13

Combine the Harvard IOP posterior with the class study posterior using `bind_rows()`. Include `ob_iop_ppd` as the first argument, and `class_ppd` as the second argument. 

```{r smaller-sample-study-13, exercise = TRUE}

```

```{r smaller-sample-study-13-hint-1, eval = FALSE}
bind_rows(..., ...)
```

### 

Our tibble now has 8,000 rows, half from the IOP poll posterior and half from the class poll posterior. Each row is one draw from the posterior distribution, equivalent to randomly sampling from the distribution of feasible Obama approval percentages 8,000 times. 

### Exercise 14

Copy your code from above, and continue your pipe using `ggplot()`. Map `approval` to the x-axis, and `poll` to the fill. Also add the layer `geom_histogram()`. 

```{r smaller-sample-study-14, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-14-hint-1, eval = FALSE}
... |>
  ggplot(aes(x = ..., fill = ...)) + 
  geom_histogram()
```

### 

Because we want to find the probability of each possible Obama approval rating, we should convert our y-axis to percentage values. We should also pick a bin size to remove the warning. 

### Exercise 15

Within `geom_histogram()`, use `aes()` as the first argument to set `y` equal to `after_stat(count/sum(count))` in order to convert the y-axis to percent values. Also set `bins` to 100, `alpha` to .5, and `position` to "identity".  

```{r smaller-sample-study-15, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-15-hint-1, eval = FALSE}
... +
  geom_histogram(aes(... = ...),
                 bins = ..., 
                 alpha = ...,
                 position = ...)
```

### 

We set `position` to "identity" and `alpha` to .5 to have the bars overlap in order to visualize the blended color, and `bins` to a large number like 100 in order to add more detail to the distribution. 

### Exercise 16

Great. Now add the layer `scale_x_continuous()` with the argument `labels` set to `scales::percent_format()`. Also add the layer `scale_y_continuous()`, with the argument `labels` also set to `scales::percent_format()`.

```{r smaller-sample-study-16, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-16-hint-1, eval = FALSE}
... +
  scale_x_continuous(... = ...) +
  scale_y_continuous(... = ...)
```

### 

To make our graph the most presentable possible, we should remove the extra unneeded decimal places for our percentages. 

### Exercise 17

Within *both* occurrences of `scales::percent_format()`, set `accuracy` to 1 in order to limit percentages to round the first decimal place. 

```{r smaller-sample-study-17, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-17-hint-1, eval = FALSE}
scale_x_continuous(labels = scales::percent_format(accuracy = ...)) +
scale_y_continuous(labels = scales::percent_format(accuracy = ...))
```

### Exercise 18

To finish, use `labs()` to give your graph the appropriate title, subtitle, and axis labels. Also add the layer `theme_light()`.

```{r smaller-sample-study-18, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

Your plot should look something like this:

```{r}
compare_p
```

### 

You have successfully created the posteriors for two different Obama approval polls. The next step is to compare them in terms of their centers and spreads. 

### Exercise 19

In one sentence, estimate the *center* of each distribution in your graph.

```{r smaller-sample-study-19}
question_text(NULL,
	message = "The distribution of the class poll is centered at about 38%, while the Harvard IOP poll is centered at about 43%.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### Exercise 20

Quantitatively approximate the centers of by taking the `mean()` of the `approval` column for both `ob_iop_ppd` and `class_ppd`.  

```{r smaller-sample-study-20, exercise = TRUE}

```

```{r smaller-sample-study-20-hint-1, eval = FALSE}
mean(ob_iop_ppd$...)
mean(class_ppd$...)
```

### 

Although you can determine the centers of each posterior using the graph visually, it's important to understand that the mean of the posterior determines its center. 

### Exercise 21

Now, let's compare the centers of our distribution with the approval percentages approximated by the  class and IOP polls. 

### 

Recall `iop_clean`, the "clean" Harvard IOP poll data for which we dropped `NA` values and turned the raw data into a binomial format: 

```{r}
iop_clean
```

### 

Use `sum()` to find the number of times the `approve` column of `iop_clean` is equal to `1`. Divide that quantity by the number of rows of `iop_clean`, for which you can use `nrow()` to calculate. 

```{r smaller-sample-study-21, exercise = TRUE}

```

```{r smaller-sample-study-21-hint-1, eval = FALSE}
sum(... == ...)/nrow(...)
```

```{r smaller-sample-study-21-hint-2, eval = FALSE}
sum(iop_clean$... == 1)/nrow(...)
```

### 

The Harvard IOP poll approximated that `r round(100*(sum(iop_clean$approve == 1)/nrow(iop_clean)), digits = 3)`% of young Americans aged 18-29 approved of President Obama. 

### Exercise 22

Now approximate the approval percentage approximated by the class poll. Recall `class_clean`, for which we "cleaned" the smaller class poll data: 

```{r}
class_clean
```

### 

Use `sum()` to find the number of times the `approve` column of `class_clean` is equal to `1`. Divide that quantity by the number of rows of `class_clean`, for which you can use `nrow()` to calculate. 

```{r smaller-sample-study-22, exercise = TRUE}

```

```{r smaller-sample-study-22-hint-1, eval = FALSE}
sum(... == ...)/nrow(...)
```

```{r smaller-sample-study-22-hint-2, eval = FALSE}
sum(class_clean$... == 1)/nrow(...)
```

### 

The class government poll approximated that `r round(100*(sum(class_clean$approve == 1)/nrow(class_clean)), digits = 3)`% of young Americans aged 18-29 approved of President Obama. 

### Exercise 23

Using the previous few exercises, what determines the *center* of a distribution? Respond in one sentence.

```{r smaller-sample-study-23}
question_text(NULL,
	message = "The sample approximation of a population parameter determines the center of a distribution. ",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

For our polling scenario, specifically the sample proportion determines the center of the distribution created from posterior draws. Whenever you see the graph of a distribution created from data, think about what the center says about the data used to create the graph. 



### Exercise 24

We can also compare *variation* in distributions from their *spreads*. 

```{r}
compare_p
```

In 2 sentences, compare the shapes of the distributions in terms of their *spreads*.

```{r smaller-sample-study-24}
question_text(NULL,
	message = "The distribution of the IOP poll is tall and narrow meaning that it has little spread.  Comparitively,  the class poll is short and wide meaning it has a large spread.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### Exercise 25

What does the difference in spreads of the Harvard IOP and class poll distributions tell you? Respond in 1 sentence, making sure to use the term *uncertainty* in your response. 

```{r smaller-sample-study-25}
question_text(NULL,
	message = "The class poll has much more uncertainty regarding the true percentage of young Americans aged 18-29 who approve of President Obama because its spread is much larger than that of the Harvard IOP poll.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### Exercise 26

Calculate the variation of each distribution by finding the standard deviation using `sd()` of the `approval` column for both `ob_iop_ppd` and `class_ppd`. 

```{r smaller-sample-study-26, exercise = TRUE}

```

```{r smaller-sample-study-26-hint-1, eval = FALSE}
sd(ob_iop_ppd$...)
sd(class_ppd$...)
```

### 

It is important to realize that the standard deviation quantifies the variation of a distribution, giving its spread. Because the standard deviation of the class poll is 3 times that of the Harvard IOP poll, it has much larger spread. 

### Exercise 27

Consider how the number of *observations* in each poll may influence the spread. Using `nrow()`, find the number of rows in both `iop_clean` and `class_clean` to determine the number of observations each poll had (without `NA` values).

```{r smaller-sample-study-27, exercise = TRUE}

```

```{r smaller-sample-study-27-hint-1, eval = FALSE}
nrow(...)
nrow(...)
```

```{r smaller-sample-study-27-hint-2, eval = FALSE}
nrow(iop_clean)
nrow(class_clean)
```

### 

The Harvard IOP poll had `r nrow(iop_clean)` observations, while the smaller class poll had more than 11 times less less observations with `r nrow(class_clean)`. 

### Exercise 28

Using the previous few exercises, what causes a distribution to have more or less *spread*? Respond in 1 sentence, making sure to use the term *observations* in your response. 

```{r smaller-sample-study-28}
question_text(NULL,
	message = "Distributions approximated with many observations have little spread, while ditributions approximated with few observations have more spread.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

There is an intuitive relationship between *spread* and the number of *observations*. With few people in a sample you are uncertain if the sample is representative of the entire population, and as such the distribution will have wider spread. As the number of people in the sample increases, then you are more certain in your estimate of the population parameter, as a larger sample is more likely to "average out".

### 

The center of a distribution approximates a parameter for an entire population, while the spread of a distribution represents the uncertainty surrounding the estimated parameter. In our polling scenario, the proportion determines the center, while the number of observations in the sample determines the spread.

## Confidence
### 

Standard error and confidence intervals are important statistical measures for any poll or experiment. 

### 

Standard error refers to the accuracy of one sample (point statistic). This is not to be confused with standard deviation, which explains the uncertainty surrounding the distribution of all possible samples. Standard error can be calculated by dividing the sample standard deviation by the square root of the sample size: 

$$ SE = \frac{\sigma}{\sqrt{n}} $$

### 

Confidence intervals quantify how many samples would fall within a specified range. The most common confidence interval is the 95% confidence interval, which is approximated by the value 2 times the standard error below the sample mean, and two times the standard error above the sample mean: 

$$ CI_{95} \approx \bar{x} \, \pm (2 \, \times SE)$$

### 

The Harvard IOP pollsters calculated that the sample proportion would fall within +/- 2.1 percentage points of the actual population proportion using a 95% confidence interval. How did they arrive at this conclusion? 

### Exercise 1

Recall `iop_clean`, the results of the Harvard IOP survey with the `NA` values dropped: 

```{r}
iop_clean
```

### 

Using the `sd()` function, calculate the standard deviation of the `approve` column of `iop_clean`. Save this number to a variable named `iop_sd`. 

```{r confidence-1, exercise = TRUE}

```

```{r confidence-1-hint-1, eval = FALSE}
iop_sd <- sd(iop_clean$...)
```

### 

We need the sample standard deviation in order to calculate the standard error: 

$$ SE = \frac{\color{green}{\sigma}}{\sqrt{n}} $$

As such, `iop_sd` represents the numerator of the above fraction. 

### Exercise 2

Now we must evaluate the square root of the sample size. Recall that `iop_clean` dropped 110 `NA` values, so the actual sample size consists of all 2,089 poll respondents, which is more than the `r nrow(iop_clean)` subjects in `iop_clean`. 

Divide `iop_sd` by the square root of 2,089 using the `sqrt()` function. 


```{r confidence-2, exercise = TRUE}

```

```{r confidence-2-hint-1, eval = FALSE}
iop_sd/sqrt(...)
```

### 

You should have an output of `r round(iop_sd/sqrt(2089), digits = 4)`, which represents the standard error of the Harvard IOP poll. Know that like standard deviation, a low standard error means that the sample estimate is likely accurate, while a high standard error means that the sample estimate and actual population parameter could vary considerably. 

### Exercise 3

Copy your code from above which produces the standard error of the IOP poll. Multiply the result by 2.

```{r confidence-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r confidence-3-hint-1, eval = FALSE}
iop_sd/sqrt(2089)*2
```

### 

Because 95% of the potential samples for a distribution will fall within 1.96 standard errors of the sample statistic, we multiply the standard error by 2 as a close approximation of the 95% confidence interval. 

### Exercise 4

Multiply your result from above by 100. 

```{r confidence-4, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r confidence-4-hint-1, eval = FALSE}
iop_sd/sqrt(2089)*2*100
```

### 

Your result should be approximately `r 100*round(iop_sd/sqrt(2089)*2, digits = 4)`, which closely approximates this statement made by the Harvard IOP pollsters: 

> The KnowledgePanel® survey of 2,089 18- to 29- year-old U.S. citizens has a margin of error
of +/– 2.1 percentage points (95% confidence level). 

### Exercise 5

To construct a 95% confidence interval, evaluate the following: 

$$ CI_{95} \approx \bar{x} \, \pm \color{green}{(2 \, \times SE)}$$

Copy your code from above, which represents the green part of the equation above. Evaluate the 95% confidence interval by adding and subtracting the value of your code above with the sample mean, $\bar{x}$, which can be found by taking the `mean()` of the `approve` column of `iop_clean`.  

```{r confidence-5, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r confidence-5-hint-1, eval = FALSE}
... - (iop_sd/sqrt(2089)*2)
... + (iop_sd/sqrt(2089)*2)
```

```{r confidence-5-hint-2, eval = FALSE}
mean(iop_clean$...) - (iop_sd/sqrt(2089)*2)
mean(iop_clean$...) + (iop_sd/sqrt(2089)*2)
```

### 

The value of the standard error of the IOP poll, `iop_se`, is relatively meaningless. However, using standard error to create a 95% confidence interval creates the following meaning: 

95% of samples of size 2,089 taken from young American adults would have a population proportion of between `r 100*round(mean(iop_clean$approve) - (iop_sd/sqrt(2089)*2), digits = 3)`% and `r 100*round(mean(iop_clean$approve) + (iop_sd/sqrt(2089)*2), digits = 3)`%. 

### Exercise 6

We can verify our confidence interval using the `ob_iop_ppd`, the posterior distribution of the population of young Americans aged 18-29 who approved of President Obama between October 29 and November 13, 2013. Use the `quantile()` function, passing in the `approval` column of `ob_iop_ppd` as the first argument, and then set the argument `probs` equal to a vector of the 2.5th, 50th, and 97.5th percentiles. 

```{r confidence-6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r confidence-6-hint-1, eval = FALSE}
quantile(ob_iop_ppd$..., probs = c(..., ..., ...))
```

```{r confidence-6-hint-2, eval = FALSE}
quantile(ob_iop_ppd$..., probs = c(.025, .5, .975))
```

### 

Remember that `ob_iop_ppd` consists of 4,000 draws from the population, which can be thought of as taking 4,000 random samples. So, the 2.5th and 97.5th percentiles can effectively represent the range for which 95% of all samples taken from young Americans aged 18-29 would fall between.  

### Exercise 7

```{r}
quantile(ob_iop_ppd$approval, probs = c(.025, .5, .975))
```

Using the printed results from above, calculate the difference between the 50th and 2.5th percentiles. Then calculate the difference between the 97.5th and 50th percentiles. 

```{r confidence-7, exercise = TRUE}

```

```{r confidence-7-hint-1, eval = FALSE}
... - ...
... - ...
```

```{r confidence-7-hint-2, eval = FALSE}
First, take the value for the 50th percentile and subtract the value for the 2.5th percentile. Then take the value for the 97.5th percentile and subtract the value for the 50th percentile. These computations are supposed to be very simple. 
```

### 

The distance between the 2.5th and 97.5th percentiles should approximate the 95% confidence interval calculated earlier. 

Also recall that the 50th percentile of a normal distribution is a close approximation of the sample mean. As such, the distance between the 2.5th and 50th percentiles, as well as the 50th and 97.5th percentiles, should approximate the statement made by the IOP pollsters: 

> The KnowledgePanel® survey of 2,089 18- to 29- year-old U.S. citizens has a margin of error
of +/– 2.1 percentage points (95% confidence level). 

## Simple prediction
### 

Posterior predictive draws have certain properties: 

- With a large enough sample size, posterior predictive distributions are normally distributed.

- The sample proportion used to create the posterior has a large effect on the predictive draws, while the number of observations used to create the posterior has a small effect.

### 

To illustrate the above properties, we will answer the following question: 

> Imagine that it is November 15, 2013. For the wider population associated with the Harvard IOP and Gov't class polls, how many people would approve of President Obama in a room of 20 people? 

### 

To answer this question, we must use `posterior_predict()` for both the IOP and Gov't class polls, using 20 observations, and compare the results:

```{r}
compare_predictions <- bind_rows(iop_predictions, class_predictions) |>
  ggplot(aes(x = total, fill = poll)) +
  geom_histogram(aes(y = after_stat(count/sum(count))), position = "dodge", alpha = .5, bins = 50) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(title = "Posterior Probability Distribution",
       subtitle = "Number of Obama approvers in a room of 20 people for 2 different polls",
       y = "Probability",
       x = "Number of Obama Approvers in Room",
       fill = "Poll")

compare_predictions
```

### Exercise 1

First, let's create the posterior predictions for the IOP poll. 

Within the `posterior_predict()` function, pass in `iop_fit` as the first argument, and then set the argument `newdata` equal to a `tibble()`. Within the `tibble()`, set `constant` to a vector that repeats the number 1, 20 times. Hint: use the function `rep()` to repeat, passing what you want to repeat as the first argument, and the number of times you want to repeat it as the second argument. 

Assign your code to a variable named `iop_predictions`. 

```{r simple-prediction-1, exercise = TRUE}

```

```{r simple-prediction-1-hint-1, eval = FALSE}
iop_predictions <- posterior_predict(..., 
                  ... = tibble(constant = ...))
```

```{r simple-prediction-1-hint-2, eval = FALSE}
iop_predictions <- posterior_predict(..., 
                  ... = tibble(constant = rep(..., ...)))
```

### 

We use pass a tibble with 20 rows into `newdata` because we want to estimate the number of red draws with a shovel size of 20. 

### Exercise 2

Copy your code from above and pipe on `as_tibble()`. Then type `iop_predictions` to view the results. 

```{r simple-prediction-2, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-2-hint-1, eval = FALSE}
... |>
  as_tibble()
```

### 

Each of the 4,000 rows represents one posterior prediction of the number of Obama approvers in a room of size 20. Each column represents the 20 individuals in the room, with 0's representing people who disapprove of President Obama, and 1's representing people who approve of him. 

### Exercise 3

Let's find the total number of people who approve of Obama in each prediction. Copy your code from above and create a column named `total` using `mutate()`. Set `total` to `rowSums(across(1:20))`, **making sure to surround both the 1 and 20 in back ticks!** Then continue your pipe and select the `total` column.

```{r simple-prediction-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-3-hint-1, eval = FALSE}
... |>
  mutate(total = ...) |>
  select(...)
```

### 

Instead of having 20 columns, one for the results of each individual, using `rowSums()` allows us to find the sum across our rows and only with the total number of Obama approvers. 

### Exercise 4

Finally, add another column called `poll`, and assign all rows the value "IOP poll" so we can compare poll predictions later on. 


```{r simple-prediction-4, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-4-hint-1, eval = FALSE}
mutate(poll = ...)
```

### Exercise 5

Great work! Now let's create the posterior predictions for the Gov't class poll. 

Because most of the code is the same, copy your code from above, making sure to change a few key features. First, change the variable name from `iop_predictions` to `class_predictions`. Next, replace `iop_fit` with `class_fit`. Finally, change the uniformly assigned name in the `poll` column from "IOP poll" to "Class poll". 

Type `class_predictions` to view the results. 

```{r simple-prediction-5, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

### 

Great work! You now have the posterior predictions for the IOP poll saved as `iop_predictions`, and the predictions from the Gov't class poll saved as `class_predictions`. 

### Exercise 6

Now combine the two polls into one tibble using `bind_rows()`. Pass in `iop_predictions` as the first argument, and `class_predictions` as the second. 

```{r simple-prediction-6, exercise = TRUE}

```

```{r simple-prediction-6-hint-1, eval = FALSE}
bind_rows(..., ...)
```

### 

Our tibble now has 8,000 rows, half from the IOP poll predictions and half from the class poll predictions. Again, each row can be thought of as a simulation of a room of 20 people, with the `total` column displaying the predicted number of Obama approvers in that room. 

### Exercise 7

Now let's graph our results. Use `ggplot()` on your code from above, and map `total` to the x-axis and `poll` to fill. Add the layer `geom_histogram()`

```{r simple-prediction-7, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-7-hint-1, eval = FALSE}
... |>
  ggplot(aes(x = ..., fill = ...)) + 
  geom_histogram()
```

### 

Because we are using `geom_histogram()`, bar height will be calculated by the number of observations for each value of `total`. 

### Exercise 8

Within `geom_histogram()`, use `aes()` to set `y` equal to `after_stat(count/sum(count))` in order to convert the y-axis to percent values. Also set `bins` to 50 and `position` to "dodge". 

```{r simple-prediction-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-8-hint-1, eval = FALSE}
... +
  geom_histogram(aes(y = ...),
                 bins = ...,
                 alpha = ...,
                 position = ...)
```

### 

Although there are only 20 possible x-values, we set the `bins` to 50 in order to create space between the bars. 

### Exercise 9

Another issue is that while we are attempting to create a posterior probability distribution, the y-axis is formatted as decimals instead of percents.

Add the layer `scale_y_continuous()` with the argument `labels` set to `scales::percent_format()`. Within `scales::percent_format()`, set `accuracy` to 1. 

```{r simple-prediction-9, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-9-hint-1, eval = FALSE}
... +
   scale_y_continuous(labels = ...)
```

```{r simple-prediction-9-hint-2, eval = FALSE}
... +
   scale_y_continuous(labels = scales::percent_format(accuracy = ...))
```

### Exercise 10

To finish, use `labs()` to give your graph the appropriate title, subtitle, and axis labels. Also add the layer `theme_light()`.

```{r simple-prediction-10, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

Your plot should look something like this:

```{r}
compare_predictions
```

### 

Great work! We can use our graph to answer the question we began with: 

> Imagine that it is November 15, 2013. For the wider population associated with the Harvard IOP and Gov't class polls, how many people would approve of President Obama in a room of 20 people? 

### Exercise 11

In 1 sentence, use evidence from your plot to support the following property of posterior predictive distributions: 

> With a large enough sample size, posterior predictive distributions are normally distributed.


```{r simple-prediction-11}
question_text(NULL,
	message = "Both posterior predictive distributions are bell-shaped, as they start with values with low probabilities, then the probability increases with each subsequent number of expected Obama approvers, and then decrease. ",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The number of trials influences how "normal" the graph appears. If we decided to increase the number of posterior predictive draws, then our distribution would appear like an even more symmetrical normal distribution

### Exercise 12

Using evidence from your graph, how do the centers and spreads of the two distributions compare? Respond in 2 sentences. 

```{r simple-prediction-12}
question_text(NULL,
	message = "The distributions are centered differently, as the class poll is centered at predicting 8 Obama approvers in the room, while the Harvard IOP poll is centered at predicting 9 Obama approvers in the room. However, the spreads of the distributions are extremely similar, as both graphs appear to have the same shape.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The sample mean or proportion has a large effect on where posterior predictive distributions are centered, which is demonstrated by how the distributions are centered differently. On the other hand, the spreads of both distributions are similar, which demonstrates that the number of observations used to create the posterior has little effect.

## Summary
### 

By completing this tutorial, you have successfully conquered multiple real world problems like a professional. You first analyzed the Data and Preceptor Table in *Wisdom* and the Population Table in *Justice* to determine whether or not the data can answer the questions. Next, you created a posterior for the Harvard IOP survey to quantify your uncertainty about the true value of a population parameter.  You then learned to describe distributions in terms of their means and variations in order to accurately describe their differences. You also learned how to evaluate a confidence interval using your data. Finally, you used your posteriors in order to predict the results of future situations. 

### 

Great work!

```{r download-answers, child = "../../child_documents/download_answers.Rmd"}

```
