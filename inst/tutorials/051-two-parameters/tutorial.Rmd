---
title: Two Parameters
author: David Kane and Mihir Kaushal
tutorial:
  id: two-parameters
output:
  learnr::tutorial:
    progressive: yes
    'allow_skip:': yes
runtime: shiny_prerendered
description: 'Chapter 5 Tutorial: Two Parameters'
---

```{r setup, include = FALSE}
library(learnr)
library(tutorial.helpers)
library(tidyverse)
library(brms)
library(tidybayes)
library(gtsummary)
library(skimr)
library(primer.data)

knitr::opts_chunk$set(echo = FALSE)
options(tutorial.exercise.timelimit = 600, 
        tutorial.storage = "local") 

ch5 <- nhanes |>
  filter(sex == "Male", age >= 18) |>
  select(height) |>
  drop_na()

fit_1 <- brm(formula = height ~ 1,
             data = ch5,
             family = gaussian(),
             silent = 2,
             refresh = 0,
             seed = 12)
```

```{r copy-code-chunk, child = system.file("child_documents/copy_button.Rmd", package = "tutorial.helpers")}
```

```{r info-section, child = system.file("child_documents/info_section.Rmd", package = "tutorial.helpers")}
```

<!-- This is the template tutorial for creating any tutorial which uses the Cardinal Virtues to answer a question given a data set. Although its primary use is for the main chapters in the Primer, it could be used for other assignments as well. The letters `XX` are used to indicate locations which require editing. Comments with instructions are interspersed. Read https://ppbds.github.io/primer/key-concepts.html for details on the Cardinal Virtues. -->

<!-- There should be a bunch of quotes for each Cardinal Virtue. All quotes are listed at the introduction of each section. Select just one for each virtue in this tutorial. -->

<!-- There are many opportunities for knowledge drops, especially after definition questions. Use them! Point out something about the details of the particular problem from the chapter. Recall that students often won't read the chapter, so we need to pull out the highlights. -->

<!-- The more that questions force students to consult the chapter, the better. -->

<!-- Key problems with current version: We need (?) to flesh out the Courage and Temperance sections. Key skills to practice each time including writing math formulas in Quarto and creating nice looking tables of regression results. -->

## Introduction

### 

This tutorial covers [Chapter 5: Two Parameters](https://ppbds.github.io/primer/two-parameters.html) of [*Preceptor’s Primer for Bayesian Data Science: Using the Cardinal Virtues for Inference*](https://ppbds.github.io/primer/) by [David Kane](https://davidkane.info/).

## Wisdom

### 

*The only true wisdom is in knowing you know nothing.* - Socrates

### Exercise 1

In your own words, describe the key components of Wisdom for working on a data science problem.

```{r wisdom-1}
question_text(NULL,
	message = "Wisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of validity, as to whether or not we can (reasonably!) assume that the two come from the same population.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Wisdom begins with the Preceptor Table. What data would we, ideally, require to answer our questions? We then explore the data that we actually have. We apply the concept of validity to ensure that the data we want and the data we have are similar enough to allow the latter to inform us about the former.

### Exercise 2

Define a Preceptor Table.

```{r wisdom-2}
question_text(NULL,
	message = "A Preceptor Table is the smallest possible table of data with rows and columns such that, if there is no missing data, it is easy to calculate the quantities of interest.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

A Preceptor Table has rows and columns of data such that, if you had them all, the calculation of the quantity of interest would be trivial.

### Exercise 3

Describe the key components of Preceptor Tables in general, without worrying about this specific problem.

```{r wisdom-3}
question_text(NULL,
	message = "The rows of the Preceptor Table are the units. The outcome is at least one of the columns. If the problem is causal, there will be at least two (potential) outcome columns. The other columns are covariates. If the problem is causal, at least one of the covariates will be a treatment.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The Preceptor Table includes: Units, Outcome, Treatment, Causal or predictive model, Covariates, and Moment in Time (This is often implicit in the question itself).

### Exercise 4

What are the units for this problem?

```{r wisdom-4}
question_text(NULL,
	message = "All the men in the world, one row per man.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The rows of the Preceptor Table are the units, the objects on which the outcome is measured.

### Exercise 5

What is/are the outcome/outcomes for this problem?

```{r wisdom-5}
question_text(NULL,
	message = "This is the variable which we are trying to explain/understand/predict. This is not the same thing as the answer to the question we have been asked. The question might, as above, be about the height of the 3rd tallest man we meet out of the next 100. But the concepts of 3rd or 100 do not appear in the Preceptor Table. Instead, height is our outcome variable. But, if we can build a model which explains/understands/predicts height, we can use that model to answer our questions.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Outcome is the variable we are trying to predict/understand/influence.

### Exercise 6

What are the covariates for this problem?

```{r wisdom-6}
question_text(NULL,
	message = "There are no (explicit) covariates in this model, although we will need to make use of variables like age and sex to construct our sample data.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

When we are looking at a “category” of units we call this a covariate. Possible covariates include, but are not limited to, sex, age, political party and almost everything else which might be associated with our data.

### Exercise 7

What are the treatments, if any, for this problem?

```{r wisdom-7}
question_text(NULL,
	message = "There are not treatment variables.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Treatment is the thing that we are trying to see the effect of. This data has no treatment.

### Exercise 8

What moment in time does the Preceptor Table refer to?

```{r wisdom-8}
question_text(NULL,
	message = "Now.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

This is often implicit in the question itself. One of our key roles as data scientists is to clarify the questions which we are asked. In this case, it seems clear that the questions refer to now, the present moment.

### Exercise 9

Write one sentence describing the data you have to answer your question.

```{r wisdom-9}
question_text(NULL,
	message = "The nhanes data set from the National Health and Nutrition Examination Survey conducted from 2009 to 2011 by the Centers for Disease Control and Prevention includes 15 variables, including physical attributes like weight and height.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Some of the covariates in this data could be variables such as age, sex and height. We will look at these more in depth later on.

### Exercise 10

Let's load the important packages.

Load the **tidyverse** package.

```{r wisdom-10, exercise = TRUE}

```

```{r wisdom-10-hint-1, eval = FALSE}
library(...)
```

```{r wisdom-10-test, include = FALSE}
library(tidyverse)
```

### 

So, what does our Preceptor Table look like? Assuming we are predicting height for every adult male on Earth at this moment in time, we would have height data for every male at least 18 years of age. This means that we would have about 4 billion rows, one for each male, along with a column for each individual’s height.

### Exercise 11

Load the **primer.data** package.

```{r wisdom-11, exercise = TRUE}

```

```{r wisdom-11-hint-1, eval = FALSE}
library(...)
```

```{r wisdom-11-test, include = FALSE}
library(primer.data)
```

### 

This Preceptor Table would extend all the way until person 4 billion-and-something. If we had this table, all of our questions could be answered with simple math and/or simulations. No inference is necessary if we have a Preceptor Table.

### Exercise 12

Load the **skimr** package.

```{r wisdom-12, exercise = TRUE}

```

```{r wisdom-12-hint-1, eval = FALSE}
library(...)
```

```{r wisdom-12-test, include = FALSE}
library(skimr)
```

### 

But what does our actual data look like? We will now start playing around with the data.

### Exercise 13

Run `glimpse()` on `nhanes`.

```{r wisdom-13, exercise = TRUE}
    
```

```{r wisdom-13-hint-1, eval = FALSE}
glimpse(nhanes)
```

```{r wisdom-13-test, include = FALSE}
glimpse(nhanes)
```

### 

The `nhanes` data set includes 15 variables, including physical attributes like weight and height. Let’s restrict our attention to three variables: age, sex and height.

### Exercise 14

Pipe `nhanes` to `select()` with `age`, `sex`, and `height` as parameters.

```{r wisdom-14, exercise = TRUE}
    
```

```{r wisdom-14-hint-1, eval = FALSE}
nhanes |> 
  select(...)
```

```{r wisdom-14-test, include = FALSE}
nhanes |> 
  select(age, sex, height)
```

### 

What are some observations about the data? Start to notice any patters or other interesting things in the data.

### Exercise 15

Continue the pipe and examine a random sample using `slice_sample()` and setting `n = 5`.

```{r wisdom-15, exercise = TRUE}
    
```

<button onclick="transfer_code(this)">

Copy previous code

</button>

```{r wisdom-15-hint-1, eval = FALSE}
... |> 
  slice_sample(...)
```

```{r wisdom-15-test, include = FALSE}
nhanes |> 
  select(age, sex, height) |> 
  slice_sample(n = 5)
```

### 

We think of both age and height as numbers. And they are numbers! But R distinguishes between “integers” and “doubles,” only the second of which allow for decimal values. In the nhanes data, age is an integer and height is a double.

### Exercise 16

Delete `slice_sample()` and instead use `glimpse()`.

```{r wisdom-16, exercise = TRUE}
    
```

<button onclick="transfer_code(this)">

Copy previous code

</button>

```{r wisdom-16-hint-1, eval = FALSE}
nhanes |> 
  select(...) |> 
  glimpse()
```

```{r wisdom-16-test, include = FALSE}
nhanes |> 
  select(age, sex, height) |> 
  glimpse()
```

### 

Be on the lookout for anything suspicious. Are there any NA’s in your data? What types of data are the columns, i.e. why is age characterized as integer instead of double? Are there more females than males?

### Exercise 17

In addition to `glimpse()`, we can run `skim()`, from the `skimr()` package, to calculate summary statistics. Delete `glimpse()` and instead use `skim()`.

```{r wisdom-17, exercise = TRUE}
    
```

<button onclick="transfer_code(this)">

Copy previous code

</button>

```{r wisdom-17-hint-1, eval = FALSE}
nhanes |> 
  select(...) |> 
  skim()
```

```{r wisdom-17-test, include = FALSE}
nhanes |> 
  select(age, sex, height) |> 
  skim()
```

### 

Interesting! There are 353 missing values of height in our subset of data. Just using `glimpse()` does not show us that. Let’s filter out the NA’s using `drop_na()`. This will delete the rows in which the value of any variable is missing. Because we want to examine height in men (not boys, nor females), let’s limit our data to only include adult males.

### Exercise 18

Pipe `nhanes` to `filter()`, making sure to only have **adult men**. Then, continue the pipe to select Height and then finally drop the NA's using `drop_na()`.

```{r wisdom-18, exercise = TRUE}
    
```

```{r wisdom-18-hint-1, eval = FALSE}
nhanes |> 
  filter(sex == "Male", age >= 18) |> 
  select(height) |> 
  drop_na()
```

```{r wisdom-18-test, include = FALSE}
nhanes |> 
  filter(sex == "Male", age >= 18) |> 
  select(height) |> 
  drop_na()
```

### 

Now let’s plot this data using `geom_histogram()`.

### Exercise 19

Pipe the previous code to ggplot, with `height` in `x` axis in the `aes()`. Add `geom_histogram()` with `bins = 50` and then add `labs()` layer with the `title`, `x` and `y` axis title, and a `caption`. Finally add `theme_classic()` to the end.

```{r wisdom-19, exercise = TRUE}
    
```

<button onclick="transfer_code(this)">

Copy previous code

</button>

```{r wisdom-19-hint-1, eval = FALSE}
... |>
  ggplot(aes(...)) + 
    geom_histogram(...) +
    labs(title = "Male Adult Height in the US in 2010",
         x = "Height (cm)",
         y = "Count",
         caption = "Source: National Health and Nutrition Examination Survey"
         ) +
    theme_classic()
```

This is what the finished graph should look like:

```{r}
nhanes |> 
  filter(sex == "Male", age >= 18) |> 
  select(height) |> 
  drop_na() |>
  ggplot(aes(x = height)) + 
    geom_histogram(bins = 50) +
    labs(title = "Male Adult Height in the US in 2010",
         x = "Height (cm)",
         y = "Count",
         caption = "Source: National Health and Nutrition Examination Survey"
         ) +
    theme_classic()
```

### 

Will the data we have — which is only for a sample of adult American men more than a decade ago — allow us to answer our questions, however roughly? Only if the assumption of validity makes sense.

### Exercise 20

In your own words, define "validity" as we use the term.

```{r wisdom-20}
question_text(NULL,
	message = "Validity is the consistency, or lack thereof, in the columns of the data set and the corresponding columns in the Preceptor Table.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

In order to consider the two data sets to be drawn from the same population, the columns from one must have a valid correspondence with the columns in the other.

### Exercise 21

What can't we do if the assumption of validity is not true?

```{r wisdom-21}
question_text(NULL,
	message = "We can't combine the Preceptor Table and the data in order to construct the Population Table.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Validity, if true (or at least reasonable), allows us to construct the Population Table. So if validity is not true, then we cannot construct the Population Table.

### Exercise 22

Provide one reason why the assumption of validity might not hold for this problem.

```{r wisdom-22}
question_text(NULL,
	message = "We need to be careful about mistakes like measurement units, like centimeters in one and inches in the other. And there can be issues like: Are measurements taken with shoes on or shoes off?",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

There are several reasons why this data might not be valid, such as inaccurate measurements. But, for the most part, the “height” variable in NHANES in 2010 is a valid proxy for the “height” of individuals today. We can stack the two data sets together and consider them to have come from the same population.

### Exercise 23

Summarize the state of your work so far in one or two sentences. Make reference to the data you have and to the question you are trying to answer.

```{r wisdom-23}
question_text(NULL,
	message = "Using the The nhanes data set from the National Health and Nutrition Examination Survey conducted from 2009 to 2011 by the Centers for Disease Control and Prevention includes 15 variables, including physical attributes like weight and height, we seek to create a model of height for adult men.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

We still haven't answered our three main questions:

What is the probability that the next man we meet will be taller than 180 centimeters?

What is the probability that, among the next 4 men we meet, the tallest is at least 10 cm taller than the shortest?

What is our posterior probability distribution for the height of the 3rd tallest man out of the next 100 we meet?

## Justice

### 

*The arc of the moral universe is long, but it bends toward justice.* - Theodore Parker

### Exercise 1

In your own words, name the four key components of Justice for working on a data science problem.

```{r justice-1}
question_text(NULL,
	message = "Justice concerns four topics: the Population Table, stability, representativeness, and unconfoundedness.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The mathematical structure of the Data Generating Mechanism. Models require math, so we need to create a mathematical formula which connects our outcome to our covariates.

### Exercise 2

In your own words, define a Population Table.

```{r justice-2}
question_text(NULL,
	message = "The Population Table includes a row for each unit/time combination in the underlying population from which both the Preceptor Table and the data are drawn.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The Population Table, a structure which includes a row for every unit in the population. We generally break the rows in the Population Table into three categories: the data for units we want to have (the Preceptor Table), the data for units which we actually have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).

### Exercise 3

Describe the Population Table for this problem. In particular, are any of the rows in the Preceptor Table also rows in the data? Are there other rows in the Population Table which are not from the Preceptor Table or the data? If so, describe some of those rows.

```{r justice-3}
question_text(NULL,
	message = "The Population Table shows rows from three sources: the Preceptor Table, the actual data, and the population (outside of the data). Our Preceptor Table rows contain entries for our covariates (sex and year) but they do not contain any outcome results (height). Our actual data rows contain entries for both our covariates and the outcomes. Our other rows contain no data. These are subjects which fall under our desired population, but for which we have no data.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Our Preceptor Table rows contain the information that we would want to know in order to answer our questions. We are trying to answer questions about the male population in 2024, so our sex entries for these rows will read “Male” and our year entries of these rows will read “2024”.

Our actual data rows contain the information that we do know. In this case, the actual data comes from a study conducted on males in 2009-2011, so our sex entries for these rows will read “Male” and our year entries of these rows will either read “2009”, “2010”, or “2011”.

Our other rows contain no data. These are subjects which fall under our desired population, but for which we have no data. As such, all outcomes and covariates are missing. (A subtle point is that, even for other data, we “know” the ID and the Year for each subject. Of course, we don’t really know these things, but, conceptually, we are defining the meaning of those rows on the basis of those variables.)

### Exercise 4

In your own words, define the assumption of "stability" when employed in the context of data science.

```{r justice-4}
question_text(NULL,
	message = "Stability means that the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

*The longer the time period covered by the Preceptor Table (and the data), the more suspect the assumption of stability becomes.* 

### Exercise 5

Provide one reason why the assumption of stability might not be true in this case.

```{r justice-5}
question_text(NULL,
	message = "A potential reason why the assumption of stability might not be true in this case is that there was a long time period covered by the data. Although it was only 3 years, this could still make the assumption of stability suspecious.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The actual data comes from a study conducted on males in 2009-2011. Stability is all about *time*. Is the relationship among the columns in the Population Table stable over time? In particular, is the relationship at the time the data was gathered the same as the relationship at the time references by the Preceptor Table?

### Exercise 6

In your own words, define the assumption of "representativeness" when employed in the context of data science.

```{r justice-6}
question_text(NULL,
	message = "Representativeness, or the lack thereof, concerns two relationship, among the rows in the Population Table. The first is between the Preceptor Table and the other rows. The second is between our data and the other rows.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Stability looks across time periods. Reprentativeness looks within time periods.

### Exercise 7

Provide one reason why the assumption of representativeness might not be true in this case.

```{r justice-7}
question_text(NULL,
	message = "A reason why the assumption of representativeness might not be true in this case is that participation is voluntary. Perhaps certain individuals (immobile, elderly, anxious) are less likely to participate. Perhaps individuals that are hospitalized do not get the opportunity to participate.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

It is important that our data is accurately reflecting the population whom we want to measure. Our data might underrepresent people who do not want to participate in the study for various reasons, this impacts our data!

### Exercise 8

In your own words, define the assumption of "unconfoundedness" when employed in the context of data science.

```{r justice-8}
question_text(NULL,
	message = "Unconfoundedness means that the treatment assignment is independent of the potential outcomes, when we condition on pre-treatment covariates.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

This assumption is only relevant for causal models. Our current data is not being used for a causal model so this assumption does not matter as much as the other ones. 

### Exercise 10

Summarize the state of your work so far in two or three sentences. Make reference to the data you have and to the question you are trying to answer. Feel free to copy from your answer at the end of the Wisdom Section. Mention at least one specific problem which casts doubt on your approach.

```{r justice-10}
question_text(NULL,
	message = "We want to create a model of height for adult men. We will then use that model to answer three questions:

What is the probability that the next man we meet will be taller than 180 centimeters?

What is the probability that, among the next 4 men we meet, the tallest is at least 10 cm taller than the shortest?

What is our posterior probability distribution for the height of the 3rd tallest man out of the next 100 we meet?

In this section, we have used the data to created a Population Table that can be used to accurately answer our questions. We looked at the stability, representativeness, and unconfoundedness to make sure this data is good to use. One thing to keep in mind is that the assumption of representativeness might not be true in this case because participation is voluntary. This will impact our data.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

## Courage

### 

*Courage is the commitment to begin without any guarantee of success.* - Johann Wolfgang von Goethe

<!-- Questions about models, tests, and the DGM. -->

### Exercise 1

<!-- DK: Not sure I like this answer. -->

In your own words, name the key goal of Courage and the process we use to get there.

```{r courage-1}
question_text(NULL,
	message = "Courage selects the data generating mechanism. We first specify the mathematical formula which connects the outcome variable we are interested in with the other data that we have. We explore different models. We need to decide which variables to include and to estimate the values of unknown parameters. We check our models for consistency with the data we have. We avoid hypothesis tests. We select one model.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

In data science, we deal with words, math, and code, but the most important of these is code. We need Courage to create the model, to take the leap of faith that we can make our ideas real.

### Exercise 2

Load the **brms** package.

```{r courage-2, exercise = TRUE}

```

```{r courage-2-hint-1, eval = FALSE}
library(...)
```

```{r courage-2-test, include = FALSE}
library(brms)
```

### 

For this section, we use a simple linear model:

$$ y_i =  \mu + \epsilon_i $$

with $\epsilon_i \sim N(0, \sigma^2)$. $y_i$ is the height of male $i$. $\mu$ is the average height of all males in the population. $\epsilon_i$ is the "error term," the difference between the height of male $i$ and the average height of all males.

$\epsilon_i$ is normally distributed with a mean of 0 and a standard deviation of $\sigma$. The mean being 0 relates to our concept of **accuracy**; we are assuming that our data is representative enough to be accurate, and so we can expect our average error to be 0. The standard deviation, on the other hand, relates to our concept of **precision**; the smaller $\sigma$ is, the more precise our data is, and the larger $\sigma$ is, the less precise our data is. 

This is the simplest model we can construct. The model has two unknown parameters: $\mu$ and $\sigma$. Before we can do anything else we need to estimate the values of these parameters. Can we ever know their exact value? No! Perfection lies only in God's own R code. But, by using a Bayesian approach, we will be able to create a *posterior probability distribution* for each parameter.

### Exercise 3

Load the **tidybayes** package.

```{r courage-3, exercise = TRUE}

```

```{r courage-3-hint-1, eval = FALSE}
library(...)
```

```{r courage-3-test, include = FALSE}
library(tidybayes)
```

### 

The parameter we most care about is $\mu$. That is the parameter with a substantively meaningful interpretation. Not only is the meaning of $\sigma$ difficult to describe, we also don't particular care about its value. Parameters like $\sigma$ in this context are *nuisance* or *auxiliary* parameters. We still estimate their posterior distributions, but we don't really care what those posteriors look like.

$\mu$ is not the average height of the men in the sample. We can calculate that directly. It is `r mean(ch5$height)`. No estimation required! Instead, $\mu$ is the average height of men in the *population*. Recall that the population is the universe of people/units/whatever about which we seek to draw conclusions. On some level, this seems simple. On a deeper level, it is very subtle. For example, if we are walking around Copenhagen, then the population we really care about, in order to answer our three questions, is the set of adult men which we might meet today. This is not the same as the population of adult men in the US in 2010. But is it close enough? Is it better than nothing? We want to assume that both men from `nhanes` (the data we have) and men we meet in Copenhagen today (the data we want to have) are drawn from the same *population*. Each case is a different and the details matter.

$\sigma$ is an estimate for the standard deviation of the errors,  i.e., variability in height after accounting for the mean.

<!-- XX: Might need an exercise which gets/creates/cleans the data you need for fitting the model. -->

### Exercise 4

Create a model using `brm()` from the **brms** package. Your arguments are `formula = height ~ 1`, `data = ch5`, `family = gaussian()`, `silent = 2`, `refresh = 0`, and `seed = 12`.

Assign the result to an object called `fit_1`.

<!-- Do not forget to create this model yourself in the setup chunk. Do this once, save the object, comment out that code and then just read_rds to create the object for this tutorial. -->

<!-- Depending on code speed, you can run this function multiple times, without assigning the return value, looking at the printout, and seeing how things change. -->

```{r courage-4, exercise = TRUE}

```

```{r courage-4-hint-1, eval = FALSE}
fit_1 <- brm(...)
```

### 

Note:

* There is a direct connection between the mathematical form of the model created under Justice and the code we use to fit the model under Courage. `height ~ 1` is the code equivalent of $y_i =  \mu$. 

* There are several ways to examine the fitted model. The simplest is to print it. Recall that just typing `x` at the prompt is the same as writing `print(x)`.

### Exercise 5

Type `fit_1` and hit "Run Code." This generates the same results as using `print(fit_1)`.

```{r courage-5, exercise = TRUE}

```

```{r courage-5-hint-1, eval = FALSE}
print(...)
```

```{r courage-5-test, include = FALSE}
print(fit_1)
```

### 

Note:

* We will be looking at this print out many times in the rest of the *Primer*. You will get used to it. The header information about Family, Links and so on just confirms what we already know. Still, you want to check this to make sure you did not make a mistake in the call to `brm()`.

* The "Estimate" of 175.87 makes sense. After all, the simple mean of the `height` in the data is:


```{r}
#| code-fold: false
mean(ch5$height)
```


### Exercise 6

Run `family()` on `fit_1`. `family()` provides information about the "family" of the error term and the link between it and the dependent variable.

```{r courage-6, exercise = TRUE}

```

```{r courage-6-hint-1, eval = FALSE}
family(...)
```

```{r courage-6-test, include = FALSE}
family(fit_1)
```

### 

In this case, setting `family = gaussian()` implies that $\epsilon_i \sim N(0, \sigma^2)$, just as we assumed. That is not a coincidence! If $\epsilon_i$ had a different distribution, we would need to use a different statistical family. 

### Exercise 7

Run `formula()` on `fit_1`. `formula()` returns the statistical equation which relates the dependent variable to the independent variable(s).

```{r courage-7, exercise = TRUE}

```

```{r courage-7-hint-1, eval = FALSE}
formula(...)
```

```{r courage-7-test, include = FALSE}
formula(fit_1)
```

### 

In this case, the key parameter is the "Intercept," which is the same thing as $\mu$ in the mathematical description of the model and the same thing as `1` when we set `formula = height ~ 1`. In other words, we are speaking three languages here: English ("Intercept"), math (\$mu$), and code (`height ~ 1`). But all three languages are referring to the same underlying concept.

### Exercise 8

Run `nobs()` on `fit_1`. The `nobs()` function returns the **n**umber of **obs**ervations.

```{r courage-8, exercise = TRUE}

```

```{r courage-8-hint-1, eval = FALSE}
nobs(...)
```

```{r courage-8-test, include = FALSE}
nobs(fit_1)
```

### 

<!-- XX -->

In this case, the original data had 10,000 rows, but remember that this current data only has adult males, it it should be less than 10,000. 

### Exercise 9

Run `posterior_interval()` on `fit_1`. The `posterior_interval()` function returns 95% intervals for all the parameters in our model.

```{r courage-9, exercise = TRUE}

```

```{r courage-9-hint-1, eval = FALSE}
posterior_interval(...)
```

```{r courage-9-test, include = FALSE}
posterior_interval(fit_1)
```

### 

<!-- XX -->

In this case, the end points of the confidence intervals are simply the estimate two times the standard error.

### Exercise 10

Run `fixef()` on `fit_1`. The `fixef()` returns information about the **fix**ed **ef**fects in the model.

```{r courage-10, exercise = TRUE}

```

```{r courage-10-hint-1, eval = FALSE}
fixef(...)
```

```{r courage-10-test, include = FALSE}
fixef(fit_1)
```

### 

In this case, XX

<!-- DK: Consider adding questions about conditional_effects(), ranef() and other commands, if relevant. -->

### Exercise 11

Run `pp_check()` on `fit_1`. The `pp_check()` runs a **p**osterior **p**redictive check.

```{r courage-11, exercise = TRUE}

```

```{r courage-11-hint-1, eval = FALSE}
pp_check(...)
```

```{r courage-11-test, include = FALSE}
pp_check(fit_1)
```

### 

In this case, note how similar our actual data, $y$, is to the 10 versions of the replicated data, $y_rep$. They are close enough that we are happy to use `fit_1`. However, the match is not perfect! The actual data is slightly more "peaked" and also features some weird bumps in the tails, especially around 200 cm. It would be possible, but not easy, to modify our model to match the actual data more closely. For now, we will just accept `fit_1` as our data generating mechanism.

<!-- If the fake data had looked very different from the real data, we have had a problem. But, for the most part, we conclude that, although not perfect, pp_check() shows that the fake outcomes generated by our model are like the actual outcome data. -->

### Exercise 12

Use `library()` to load the [**gtsummary**](https://www.danieldsjoberg.com/gtsummary) package.

```{r courage-12, exercise = TRUE}

```

```{r courage-12-hint-1, eval = FALSE}
library(gtsummary)
```

```{r courage-12-test, include = FALSE}
library(gtsummary)
```

### 

We modeled `height`, a continuous variable measured in centimeters, as a linear function of a constant term. The average adult male height in the US was around 176 cm. Mathematically:

$$ height_i =  176 + \epsilon_i $$

with $\epsilon_i \sim N(0, 0.75^2)$.

### Exercise 13

<!-- DK: This can be just one question or several, especially if you want to teach some more gtsummary or gt tricks. Note that you need to adjust the test to make sure it works. -->

Run `tbl_regression()` on `fit_1`.

```{r courage-13, exercise = TRUE}

```

```{r courage-13-hint-1, eval = FALSE}
tbl_regression(...)
```

```{r courage-13-test, include = FALSE}
# tbl_regression(fit_1)
```

### 

<!-- DK: Give something like the same sentence which you want the students to use at the end of the next question. XX -->

### Exercise 14

Write a few sentence which summarize your work so far. The first few sentences are the same as what you had at the end of the Justice Section. Add at least one sentence which describes the modelling approach which you are using, specifying at least the functional form and the dependent variable. Add at least one sentence which describes the *direction* (not the magnitude) of the relationship between one of your independent variables and your dependent variable.

```{r courage-14}
question_text(NULL,
	message = "XX",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

## Temperance

### 

*Temperance is a tree which as for its root very little contentment, and for its fruit calm and peace.* - Buddha 

### Exercise 1

In your own words, describe the use of Temperance in finishing your data science project.

```{r temperance-1}
question_text(NULL,
	message = "Temperance guides us in the use of the data generating mechanism --- or the 'model' ---  we have created to answer the questions with which we began. We create posteriors for the quantities of interest.",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

We should be modest in the claims we make. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.

### Exercise 2

Recall the question with which we began:

> ...

Pipe `fit_1` to . . .

```{r temperance-2, exercise = TRUE}

```

```{r temperance-2-hint-1, eval = FALSE}

```

```{r temperance-2-test, include = FALSE}

```

### 

### Exercise 3

Write a paragraph which summarizes the project in your own words. The first few sentences are the same as what you had at the end of the Courage Section. But, since your question may have evolved, you should feel free to change those sentences. Add at least one sentence which describes at least one quantity of interest (QoI) --- presumably one that answers your question -- and which provides a measure of uncertainty about that QoI.

```{r temperance-3}
question_text(NULL,
	message = "XX",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

### Exercise 4

Write a few sentences which explain why the estimates for the quantities of interest, and the uncertainty thereof, might be wrong. Suggest an alternative estimate and confidence interval.

```{r temperance-4}
question_text(NULL,
	message = "XX",
	answer(NULL, correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

## Summary

### 

This tutorial covered [Chapter XX: XX](https://ppbds.github.io/primer/XX.html) of [*Preceptor’s Primer for Bayesian Data Science: Using the Cardinal Virtues for Inference*](https://ppbds.github.io/primer/) by [David Kane](https://davidkane.info/).

```{r download-answers, child = system.file("child_documents/download_answers.Rmd", package = "tutorial.helpers")}
```
